{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all!\n",
    "In this script, I try to use (deep) reinforcement learning (a2c) to solve the knapsack problem.\n",
    "This should not be an exciting project but just a warm up (for me at least) to understand hands-on RL, REINFORCE, $\\epsilon$-greedy, embeddings and such.\n",
    "\n",
    "The idea is to use Q learning to solve the knapsack probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the knapsack problem (KP)? \n",
    "The KP is a problem where you are given a knapsack (a bag) with limited capacity and a set of objets.\n",
    "Objects have two attributes, prize and weight. Your goal is to select some objects such that you maximize the prize of the object chosen without exceeding the capacity of the knapsack.\n",
    "\n",
    "For example, given a knapsack of capacity 1 (also in the following, we assume weights to be normalized, so capacity knapsack = 1) and objects [prize, weight] = {[3,0.8],[1,0.25],[1,0.25],[1,0.25],[1,0.25]} the best solution would be to NOT pick up $obj_{0}$ (even if it has the greatest prize) while picking up all the other objects.\n",
    "\n",
    "Why I've chosen the KP? Because the KP is often considered the easiet among the NP-Hard problem (meaning that it takes exponential number of steps to achieve the optimal solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import random\n",
    "import itertools  \n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ObjectCass:\n",
    "    \n",
    "    Prize = None\n",
    "    Weight = None\n",
    "    \n",
    "    def __init__(self, reward, weight):\n",
    "        \n",
    "        self.Prize = reward\n",
    "        self.Weight = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the enviroment function.\n",
    "The environment generates num_objs (input) objects. \n",
    "The enviroment can either generate a real instance (by default) or a fake one of which we know the optimal solution.\n",
    "\n",
    "We add a fake object of weight and prize zero. This works as a 'final token'. When the algorithm sees this, it stops adding objects. The idea is that a good algorithm will add objects until the capacity is almost fully reached and then select this fake object to break the cycle.\n",
    "\n",
    "The environment returns the objects and their target.\n",
    "The target is computed via an heuristic and then it is scaled by something smaller than 1. We scale the target in order to have positive 'rewards' (meaning reward-target to be positive).\n",
    "\n",
    "For the fake instance, the odd objects have prize 1 and weight (2/num_objs)- epsilon (epsilon being a small numer), while the even ones have price 0.1 and weight 0.99. Since the capacity is normalized, the best solution is to choose all and only the even objects (i.e. capacity = num_objects/2 * (2/num_objs)- epsilon < 1 and reward = num_objects/2 ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('s_0 cap', tensor([1.]))\n",
      "('s_1 cap', tensor([0.5500]))\n",
      "('s_0 price', tensor(0.))\n",
      "('s_1 price', tensor(0.9700))\n",
      "('s_0 obj features', tensor([[0.9700, 0.4500, 1.0000],\n",
      "        [0.0200, 0.9100, 1.0000]]))\n",
      "('s_1 obj features', tensor([[0.9700, 0.4500, 0.5500],\n",
      "        [0.0200, 0.9100, 0.5500]]))\n",
      "('reward', 0.97)\n"
     ]
    }
   ],
   "source": [
    "def Heuristic(Objects, scaling_factor=1 ):\n",
    "    \n",
    "    special_object = Objects[-1]\n",
    "    Objects.remove(special_object)\n",
    "    \n",
    "    Objects_sorted = sorted(Objects, key=lambda obj: -float(obj.Prize)/obj.Weight)\n",
    "    weight_total = 0\n",
    "    price_total = 0\n",
    "    for obj in Objects_sorted:\n",
    "        if weight_total+obj.Weight>1:\n",
    "            continue\n",
    "        price_total+=obj.Prize\n",
    "        weight_total+=obj.Weight\n",
    "    Objects.append(special_object)\n",
    "    \n",
    "    target = torch.tensor(price_total).float()\n",
    "    target = scaling_factor*target # sclaed target to have positive reward for good actions\n",
    "    target.requires_grad = False\n",
    "    \n",
    "    return target\n",
    "\n",
    "class StateClass:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    res_capacity = None\n",
    "    Chosen = None\n",
    "    price = None\n",
    "    Final = None\n",
    "    Selectable = None\n",
    "    SelectableObjectsFeatures = None\n",
    "    \n",
    "    def __init__(self, ObjectsFeatures, res_capacity, Chosen, price, env):\n",
    "        \n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        self.res_capacity = res_capacity\n",
    "        self.Chosen = Chosen\n",
    "        self.price = price\n",
    "        self.Final = 0\n",
    "        self.Selectable = self.StateMaskFunction(env)\n",
    "        if len(self.Selectable)==0:\n",
    "            self.Final=1\n",
    "        self.SelectableObjectsFeatures = self.SelectableObjectsFeaturesFunction()\n",
    "    \n",
    "        return\n",
    "\n",
    "    def StateMaskFunction(self, env):\n",
    "        \n",
    "        indexes = [i for i in range(len(env.Objects)) \n",
    "            if i not in self.Chosen and \n",
    "            not (self.res_capacity - env.Objects[i].Weight < 0)]\n",
    "    \n",
    "        return indexes \n",
    "    \n",
    "    def SelectableObjectsFeaturesFunction(self):\n",
    "        \n",
    "        SelectableObjectsFeatures = []\n",
    "        for i in range(len(self.ObjectsFeatures)):\n",
    "            if i in self.Selectable:\n",
    "                SelectableObjectsFeatures.append(self.ObjectsFeatures[i])\n",
    "        \n",
    "        if len(SelectableObjectsFeatures)>0:\n",
    "            SelectableObjectsFeatures = torch.stack(SelectableObjectsFeatures)\n",
    "        \n",
    "        return SelectableObjectsFeatures\n",
    "\n",
    "class Environment:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    Objects = None\n",
    "    capacity = None\n",
    "    target = None\n",
    "    \n",
    "    def __init__(self, num_objs, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, scaling_factor = 1, shuffle = False):\n",
    "        \n",
    "        if FakeBool:\n",
    "            # fake code to control the instance I fed it.\n",
    "            # best solution = [1,0,1,0,1,0,1,0,1,0,1]\n",
    "            Objects = []\n",
    "            for i in range(num_objs):\n",
    "                if i%2==1:\n",
    "                    prize = 0.1\n",
    "                    weight = 0.99\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "                else:\n",
    "                    prize = 1\n",
    "                    weight = (2/float(num_objs))-0.001\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "            if shuffle:\n",
    "                random.shuffle(Objects)\n",
    "            target = torch.tensor(int(num_objs/2)).float() # real target\n",
    "            target = scaling_factor*target # sclaed target to have positive reward for good actions\n",
    "            target.requires_grad = False\n",
    "            ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "            ObjectsFeatures.requires_grad = False            \n",
    "\n",
    "        else:\n",
    "            Objects = []\n",
    "            for i in range(num_objs):\n",
    "                prize = round(random.uniform(prize_min, prize_max), 2)\n",
    "                weight = round(random.uniform(weight_min, weight_max), 2)\n",
    "                obj = ObjectCass(prize,weight)\n",
    "                Objects.append(obj)\n",
    "            target = Heuristic(Objects, scaling_factor)\n",
    "            ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "            ObjectsFeatures.requires_grad = False\n",
    "\n",
    "        capacity = torch.tensor([1]).float() #\n",
    "        capacity.requires_grad = False\n",
    "\n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        self.Objects = Objects\n",
    "        self.capacity = capacity\n",
    "        self.target = target\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def CreateInitialState(self):\n",
    "        \n",
    "        price = torch.tensor(0).float() #\n",
    "        price.requires_grad = False\n",
    "        ObjectsFeatures = []\n",
    "        for obj in self.ObjectsFeatures:\n",
    "            ObjectsFeatures.append(torch.cat([obj, self.capacity]))\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures)\n",
    "        \n",
    "        return StateClass(ObjectsFeatures, self.capacity, [], price, self)\n",
    "\n",
    "    def step(self, state_old, obj_chosen):\n",
    "\n",
    "        capacity = state_old.res_capacity-state_old.ObjectsFeatures[obj_chosen][1].item()\n",
    "        price = state_old.price+state_old.ObjectsFeatures[obj_chosen][0].item()\n",
    "        New_Chosen = [old_chosen for old_chosen in state_old.Chosen]\n",
    "        New_Chosen.append(obj_chosen)\n",
    "        ObjectsFeatures = state_old.ObjectsFeatures.clone()\n",
    "        for i in range(len(ObjectsFeatures)):\n",
    "            ObjectsFeatures[i][-1] = capacity\n",
    "        state_new = StateClass(ObjectsFeatures, capacity, New_Chosen, price, self)\n",
    "        \n",
    "        # new state , reward\n",
    "        return state_new, env.Objects[obj_chosen].Prize\n",
    "    \n",
    "    def RandomPicker(self):\n",
    "        \n",
    "        Chosen = []\n",
    "        res_cap = 0\n",
    "        possible_objs = [obj for obj in self.Objects if obj.Weight + res_cap < self.capacity and obj not in Chosen]\n",
    "        price = 0\n",
    "        while len(possible_objs)>0:\n",
    "            obj = random.choice(possible_objs)\n",
    "            res_cap+=obj.Weight\n",
    "            price+=obj.Prize\n",
    "            Chosen.append(obj)\n",
    "            possible_objs = [obj for obj in self.Objects if obj.Weight + res_cap < self.capacity and obj not in Chosen]\n",
    "        \n",
    "        return price\n",
    "\n",
    "env = Environment(2)\n",
    "s_0 = env.CreateInitialState()\n",
    "s_1, reward = env.step(s_0,0)\n",
    "print('s_0 cap', s_0.res_capacity)\n",
    "print('s_1 cap', s_1.res_capacity)\n",
    "print('s_0 price', s_0.price)\n",
    "print('s_1 price', s_1.price)\n",
    "print('s_0 obj features', s_0.ObjectsFeatures)\n",
    "print('s_1 obj features', s_1.ObjectsFeatures)\n",
    "print('reward', reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation based on: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a NN which uses an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs):    \n",
    "        \n",
    "        super(Net_Attention, self).__init__()\n",
    "        \n",
    "        self.emb = torch.nn.Linear(num_input_features,  h_model)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=h_model, nhead=num_head, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=p_dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "                \n",
    "        # FINAL LAYER\n",
    "        self.final_layer_0 = torch.nn.Linear(h_model,h_model)\n",
    "        self.final_layer_1 = torch.nn.Linear(h_model,1)\n",
    "\n",
    "    def forward(self, States, requires_grad = True):\n",
    "        \n",
    "        ObjectsFeatures = [state.SelectableObjectsFeatures for state in States]\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures)\n",
    "        ObjectsFeatures.requires_grad = requires_grad # in case you don't need to compute the gradient\n",
    "        \n",
    "        # define relu operation\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        \n",
    "        E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "        # where \n",
    "        # S is the source sequence length,  \n",
    "        # N is the batch size,  \n",
    "        # E is the feature number        \n",
    "        # you want \n",
    "        # E.size() = S,N,E = num_obj x batch x h_model\n",
    "        \n",
    "        c = self.transformer_encoder(E)\n",
    "        #c = torch.squeeze(c, dim = 0)        \n",
    "        #print('\\n\\n\\n')\n",
    "        #print('c')\n",
    "        #print(c)\n",
    "        c = self.final_layer_0(c)\n",
    "        c = ReLU(c)\n",
    "        Qvals = self.final_layer_1(c)\n",
    "        #print('Qvals')\n",
    "        #print(Qvals)\n",
    "        Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "        Qvals = torch.transpose(Qvals,0,1)\n",
    "        \n",
    "        return Qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that, given a state, it returns the Q values. We have to define the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrintQvals(Qvals_print,  stringa = ''):\n",
    "    \n",
    "    print('printing just 100 points for readibility reasons')\n",
    "    plt.figure()\n",
    "    for i in range(len(Qvals_print[0])):\n",
    "        ObjectOutput = [Qvals_print[j][i] for j in range(len(Qvals_print))]\n",
    "        plt.plot(ObjectOutput, label='act_'+str(i))\n",
    "    Max = []\n",
    "    for i in range(len(Qvals_print)):\n",
    "        Max.append(torch.max(Qvals_print[i]))\n",
    "    plt.plot(Max, label='Qmax')\n",
    "    plt.ylabel('Qvals' + stringa)\n",
    "    plt.legend()\n",
    "    return\n",
    "\n",
    "def MaskFunctionArray(States, Environments, Qvalss):\n",
    "    \n",
    "    QI = [MaskFunction(States[i], Environments[i], Qvalss[i]) for i in range(len(States))]\n",
    "    Qvalss_mask = [q[0] for q in QI]\n",
    "    indexess = [q[1] for q in QI]\n",
    "    \n",
    "    #Qvalss_mask = []\n",
    "    #indexess = []\n",
    "    #for i in range(len(States)):\n",
    "    #    q,i = MaskFunction(States[i], Environments[i], Qvalss[i])\n",
    "    #    Qvalss_mask.append(q)\n",
    "    #    indexess.append(i)\n",
    "        \n",
    "    return Qvalss_mask, indexess\n",
    "\n",
    "def MaskFunction(state, env, Qvals):\n",
    "        \n",
    "    indexes = [i for i in range(len(env.Objects)) \n",
    "               if i not in state.Chosen and \n",
    "               not (state.res_capacity - env.Objects[i].Weight < 0)]\n",
    "    Qvals_out = [Qvals[i] for i in indexes]\n",
    "    if len(Qvals_out)!=0:\n",
    "        Qvals_out = torch.stack(Qvals_out)\n",
    "    \n",
    "    return Qvals_out, indexes\n",
    "\n",
    "class BufferClass:\n",
    "    \n",
    "    replay_lenght = None\n",
    "    minibatch_size = None\n",
    "    Buffer = None\n",
    "    \n",
    "    def __init__(self,replay_lenght, minibatch_size):\n",
    "        \n",
    "        self.replay_lenght = replay_lenght\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.Buffer = []\n",
    "        \n",
    "    def Add(self, state, action, reward, new_state):\n",
    "                \n",
    "        while len(self.Buffer)>=self.replay_lenght:\n",
    "            self.Buffer.remove(random.choice(self.Buffer))\n",
    "        self.Buffer.append([state, action, reward, new_state])\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def Minibatch(self):\n",
    "        \n",
    "        if len(self.Buffer) <= self.minibatch_size:\n",
    "            Elements = self.Buffer\n",
    "        else:\n",
    "            Elements = random.sample(self.Buffer, minibatch_size)\n",
    "            Minibatch = Elements\n",
    "\n",
    "        #States = [experience[0] for experience in Elements]\n",
    "        #Actions = [experience[1] for experience in Elements]\n",
    "        #Rewards = [experience[2] for experience in Elements]\n",
    "        #New_States = [experience[3] for experience in Elements]\n",
    "        #\n",
    "        #Minibatch = [States, Actions, Rewards, New_States]\n",
    "        \n",
    "        return Minibatch\n",
    "\n",
    "def Learning_step(Net_predict,Net_target,optimizer, Minibatch, gamma = 1):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Losses = []\n",
    "    for experience in Minibatch:\n",
    "        state, action, reward, new_state = experience\n",
    "        action_index = state.Selectable.index(action)\n",
    "        if not new_state.Final==1:\n",
    "            loss = (reward + gamma*torch.max(Net_target([new_state], requires_grad = False).squeeze(dim = 0)) - Net_predict([state]).squeeze(dim = 0)[action_index])**2\n",
    "        else:\n",
    "            loss = (reward - Net_predict([state]).squeeze(dim = 0)[action_index])**2\n",
    "        Losses.append(loss)\n",
    "    Losses = torch.stack(Losses)\n",
    "    \n",
    "    #States, Actions, Rewards, New_States = Minibatch\n",
    "    #Max_New_States, _ = torch.max(Net_target(New_States, requires_grad = False), dim = 1)\n",
    "    #Rewards = torch.tensor(Rewards)\n",
    "    #Predictions_Actions = [Net_predict(States)[i][Actions[i]] for i in range(len(Net_predict(States)))]\n",
    "    #Predictions_Actions = torch.stack(Predictions_Actions)\n",
    "    #FinalStateBools = [state.Final for state in States]\n",
    "    #FinalStateBools = torch.tensor(FinalStateBools).float()\n",
    "    #print(FinalStateBools)\n",
    "    #stoptocheck\n",
    "    #Ones = torch.ones_like(FinalStateBools).float()\n",
    "    #Losses = (Rewards + gamma*(Ones - FinalStateBools)*Max_New_States - Predictions_Actions)**2\n",
    "    \n",
    "    loss = Losses.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping\n",
    "    clip_value = 1\n",
    "    for p in Net_predict.parameters():\n",
    "        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "    \n",
    "    # apply gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.tolist()\n",
    "\n",
    "def Evaluate(num_epoch, Net_predict, num_objs, FakeBool = False, shuffle = False, times = 1, images = True, examples = 0):\n",
    "    \n",
    "        \n",
    "    better = 0\n",
    "    for time_counter in range(1,times+1):\n",
    "        print('evaluating model..('+str(time_counter)+' out of '+str(times)+')')\n",
    "        # set to train\n",
    "        Net_predict.eval()\n",
    "        # define the number of epochs\n",
    "        T = []\n",
    "        R = []\n",
    "        R_random = []\n",
    "        for e_counter in range(num_epoch):\n",
    "            env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffle)\n",
    "            state = env.CreateInitialState()\n",
    "            while True:\n",
    "                Qvals = Net_predict([state])\n",
    "                Qvals = torch.squeeze(Qvals, dim = 0)\n",
    "                action = torch.argmax(Qvals)\n",
    "                action_index = state.Selectable[action]\n",
    "                new_state, reward = env.step(state, action_index)\n",
    "                state = new_state\n",
    "                # break if no more possible actions\n",
    "                if state.Final==1:\n",
    "                    break\n",
    "\n",
    "            T.append(env.target)\n",
    "            R.append(new_state.price)\n",
    "            R_random.append(env.RandomPicker())\n",
    "\n",
    "        R_norm = [R[i]/T[i] for i in range(len(R))]\n",
    "        R_random_norm = [R_random[i]/T[i] for i in range(len(R_random))]\n",
    "        T_norm = [1 for i in range(len(R))]\n",
    "        \n",
    "        if images:\n",
    "            plt.figure()\n",
    "            plt.plot(R)\n",
    "            plt.plot(R, label='Rewards')\n",
    "            plt.plot(R_random)\n",
    "            plt.plot(R_random, label='Rewards (random picker)')\n",
    "            plt.plot(T)\n",
    "            plt.plot(T, label='baseline')\n",
    "            plt.ylabel(' Rewards ' )\n",
    "            plt.legend()\n",
    "            plt.ylim(bottom=0)\n",
    "            plt.figure()\n",
    "            plt.plot(R_norm)\n",
    "            plt.plot(R_norm, label='Rewards (normalized)')\n",
    "            plt.plot(R_random_norm)\n",
    "            plt.plot(R_random_norm, label='Rewards (random picker) normalized')\n",
    "            plt.plot(T_norm)\n",
    "            plt.plot(T_norm, label='baseline (normalized)')\n",
    "            plt.ylabel(' Rewards (normalized)' )\n",
    "            plt.legend()\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "        print('average norm reward (net)   : ', np.mean(R_norm))\n",
    "        print('average norm reward (random): ', np.mean(R_random_norm))\n",
    "        if np.mean(R_norm)>np.mean(R_random_norm):\n",
    "            better+=1\n",
    "    print('\\n\\n\\n')\n",
    "    print('the NN was better than the random pikcer '+str(better)+' times out of '+str(times))            \n",
    "    print('\\n\\n\\nExample:')\n",
    "    if examples>0:\n",
    "        for example_counter in range(examples):\n",
    "            reward_example = 0\n",
    "            env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffle)\n",
    "            print('Objects: ')\n",
    "            for i in range(len(env.Objects)):\n",
    "                print('object num '+str(i)+' price: '+str(env.Objects[i].Prize)+' weight: '+str(env.Objects[i].Weight))\n",
    "            print('heuristic target: ', env.target)\n",
    "            state = env.CreateInitialState()\n",
    "            while True:\n",
    "                Qvals = Net_predict([state])\n",
    "                Qvals = torch.squeeze(Qvals, dim = 0)\n",
    "                action = torch.argmax(Qvals)\n",
    "                action_index = state.Selectable[action]\n",
    "                print('NN selects: ', action_index)\n",
    "                new_state, reward = env.step(state, action_index)\n",
    "                state = new_state\n",
    "                reward_example+= env.Objects[action_index].Prize\n",
    "                # break if no more possible actions\n",
    "                if state.Final==1:\n",
    "                    break\n",
    "            print('NN reward ', reward_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the whole RL framework with the A2C.\n",
    "We feed to the algorithm a fake instance and it has to choose just one action to perform.\n",
    "This is the simplest form where the error still appears.\n",
    "By 'fake' instance, I mean that we feed always the same instance and the best action is always the same. This should be extremely easy to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_input_features = 3 # prize, weight, res_capacity \n",
    "num_objs = 10\n",
    "num_outputs = num_objs\n",
    "h_model = 4\n",
    "num_head = 2\n",
    "num_layers = 2\n",
    "dim_feedforward = 4\n",
    "p_dropout = 0.1\n",
    "\n",
    "Net_predict = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "Net_target = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fake Instance?\n",
    "FakeBool = False\n",
    "shuffleBool = True\n",
    "# define the number of epochs\n",
    "num_epoch = 1000\n",
    "# epsilon\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "#\n",
    "minibatch_size = 8\n",
    "# C (how often update Net_target)\n",
    "C = max(int(num_epoch/100),100)\n",
    "# unit (for printing)\n",
    "unit = max(int(num_epoch/100),1)\n",
    "PATH_predict = '/home/big_bamboo/Downloads/Net_Predict'\n",
    "PATH_target = '/home/big_bamboo/Downloads/Net_Target'\n",
    "\n",
    "# is it a new test?\n",
    "LoadTest = False\n",
    "if LoadTest:\n",
    "    Net_predict.load_state_dict(torch.load(PATH_predict))\n",
    "    Net_target.load_state_dict(torch.load(PATH_target))\n",
    "    with open('RLKP-num_epoch_start-Save.pkl', 'rb') as input:\n",
    "        num_epoch_start = pickle.load(input)\n",
    "    with open('RLKP-Replay-buffer-Save.pkl', 'rb') as input:\n",
    "        ReplayBuffer = pickle.load(input)\n",
    "    with open('RLKP-Qvals_print_init-Save.pkl', 'rb') as input:\n",
    "        Qvals_print_init = pickle.load(input)\n",
    "    with open('RLKP-Qvals_print_final-Save.pkl', 'rb') as input:\n",
    "        Qvals_print_final = pickle.load(input)\n",
    "    with open('RLKP-optimizer-Save.pkl', 'rb') as input:\n",
    "        optimizer = pickle.load(input)\n",
    "else:\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.SGD(Net_predict.parameters(), lr=1e-3, momentum = 0.9)\n",
    "    # replay buffer\n",
    "    replay_lenght = 1e6\n",
    "    ReplayBuffer = BufferClass(replay_lenght,minibatch_size)\n",
    "    Qvals_print_init = []\n",
    "    Qvals_print_final = []\n",
    "    num_epoch_start = 0\n",
    "\n",
    "def SaveModel(Net_predict,Net_target,ReplayBuffer, e_counter, Qvals_print_init, Qvals_print_final):\n",
    "    \n",
    "    torch.save(Net_predict.state_dict(), PATH_predict)\n",
    "    torch.save(Net_target.state_dict(), PATH_target)\n",
    "    with open('RLKP-num_epoch_start-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(e_counter, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Replay-buffer-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(ReplayBuffer, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Qvals_print_init-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Qvals_print_init, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Qvals_print_final-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Qvals_print_final, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-optimizer-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(optimizer, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# optimizer scheduler\n",
    "lmbda = lambda epoch: 0.5\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EpsGreedy(Qvals,indexes, e_counter):\n",
    "    \n",
    "    # select action\n",
    "    epsilon = (epsilon_max-epsilon_min)*(num_epoch-e_counter)/num_epoch + epsilon_min\n",
    "    if random.random() < epsilon:\n",
    "        action_index = random.choice(indexes)\n",
    "    else:\n",
    "        action = torch.argmax(Qvals)\n",
    "        action_index = indexes[action]\n",
    "    \n",
    "    return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch: ', 0)\n",
      "('epoch: ', 100)\n",
      "('epoch: ', 200)\n",
      "('epoch: ', 300)\n",
      "('epoch: ', 400)\n",
      "('epoch: ', 500)\n",
      "('epoch: ', 600)\n",
      "('epoch: ', 700)\n",
      "('epoch: ', 800)\n",
      "('epoch: ', 900)\n"
     ]
    }
   ],
   "source": [
    "# set to train\n",
    "Net_predict.train()\n",
    "\n",
    "for e_counter in range(num_epoch_start, num_epoch + minibatch_size):\n",
    "    if (e_counter- minibatch_size)%100==0 and e_counter!=0:\n",
    "        print('epoch: ',(e_counter- minibatch_size))\n",
    "        # save model\n",
    "        #SaveModel(Net_predict,Net_target,ReplayBuffer, e_counter, Qvals_print_init, Qvals_print_final)\n",
    "    # creating instance\n",
    "    env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffleBool)\n",
    "    state = env.CreateInitialState()\n",
    "    while True:\n",
    "        # Compute Qvals\n",
    "        Qvals = Net_predict([state])\n",
    "        Qvals = Qvals.squeeze() # reduce dimension\n",
    "        # select action\n",
    "        action_index = EpsGreedy(Qvals,state.Selectable, e_counter)\n",
    "        # environment step\n",
    "        new_state, reward = env.step(state, action_index)\n",
    "        # replay buffer\n",
    "        ReplayBuffer.Add(state, action_index, reward, new_state)\n",
    "        state = new_state\n",
    "        # break if no more possible actions\n",
    "        if state.Final==1:\n",
    "            break\n",
    "        \n",
    "    if (e_counter- minibatch_size)>= minibatch_size:\n",
    "        l = Learning_step(Net_predict,Net_target,optimizer, ReplayBuffer.Minibatch())\n",
    "    #if (e_counter- minibatch_size)%C == 0:\n",
    "    #    Net_target.load_state_dict(Net_predict.state_dict())\n",
    "    #if (e_counter- minibatch_size)%int(num_epoch/5)==0 and e_counter!=minibatch_size:\n",
    "    #    scheduler.step()\n",
    "    #    for param_group in optimizer.param_groups:\n",
    "    #        print('learning rate ', param_group['lr'])\n",
    "    #        if param_group['lr']<1e-10:\n",
    "    #            print('the lr dropped too low, something is wrong')\n",
    "    #            error\n",
    "\n",
    "Evaluate(10, Net_predict, num_objs, FakeBool = FakeBool, shuffle = shuffleBool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Evaluate(100, Net_predict, num_objs, FakeBool = False, shuffle = shuffleBool, times = 10, images = False, examples = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO NOW:\n",
    "- if you solve with FakeBool = True and then with FakeBool = False (both with 250 iterations), it achieves very good solutions. Otherwise it doesn't. why?? (MOst likely because you re-used the same NN of before)\n",
    "- try to print Q vals\n",
    "\n",
    "STILL TO  DO:\n",
    "- investigate why it is still so slow the training (compare it witht he evaluation)\n",
    "- find a way to make minibatch_size = 1024 without making everything crash\n",
    "- make sure that the newest sequences are in the minibatch\n",
    "- put res_capacity as a repeated object feature in the state (now it is disregarded) # DONE\n",
    "- feed the NN just the releveant objects # DONE!\n",
    "- compute losses in (mini)batches -> this does not work if you feed only the relevant objects because you can't stack them (they all have different dimensions)\n",
    "- create environment in batches\n",
    "\n",
    "In the def Add() function there is an error. There is a non-zero probability that you add a new state (N_0) then add a new state (N_1) and doing so you delete N_0 with probability 1/replay_buffer_length. This is no bueno if you conside that afterwards you might have to mark as final state the stae N_0 which now might not be in the buffer anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
