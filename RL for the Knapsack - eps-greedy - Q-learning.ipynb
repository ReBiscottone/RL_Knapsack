{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all!\n",
    "In this script, I try to use (deep) reinforcement learning (a2c) to solve the knapsack problem.\n",
    "This should not be an exciting project but just a warm up (for me at least) to understand hands-on RL, $\\epsilon$-greedy, embeddings and such.\n",
    "\n",
    "The idea is to use deep Q learning to solve the knapsack probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the knapsack problem (KP)? \n",
    "The KP is a problem where you are given a knapsack (a bag) with limited capacity and a set of objets.\n",
    "Objects have two attributes, prize and weight. Your goal is to select some objects such that you maximize the prize of the object chosen without exceeding the capacity of the knapsack.\n",
    "\n",
    "For example, given a knapsack of capacity 1 (also in the following, we assume weights to be normalized, so capacity knapsack = 1) and objects [prize, weight] = {[3,0.8],[1,0.25],[1,0.25],[1,0.25],[1,0.25]} the best solution would be to NOT pick up $obj_{0}$ (even if it has the greatest prize) while picking up all the other objects.\n",
    "\n",
    "Why I've chosen the KP? Because the KP is often considered the easiet among the NP-Hard problem (meaning that it takes exponential number of steps to achieve the optimal solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ObjectCass:\n",
    "    \n",
    "    Prize = None\n",
    "    Weight = None\n",
    "    \n",
    "    def __init__(self, reward, weight):\n",
    "        \n",
    "        self.Prize = reward\n",
    "        self.Weight = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the enviroment function.\n",
    "The environment generates num_objs (input) objects. \n",
    "The enviroment can either generate a real instance (by default) or a fake one of which we know the optimal solution.\n",
    "\n",
    "The environment returns the objects and their target (which is computed via an heuristic).\n",
    "\n",
    "For the fake instance, the odd objects have prize 1 and weight (2/num_objs)- epsilon (epsilon being a small numer), while the even ones have price 0.1 and weight 0.99. Since the capacity is normalized, the best solution is to choose all and only the even objects (i.e. capacity = num_objects/2 * (2/num_objs)- epsilon < 1 and reward = num_objects/2 ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('s_0 cap', tensor([1.]))\n",
      "('s_1 cap', tensor([0.5500]))\n",
      "('s_0 price', tensor(0.))\n",
      "('s_1 price', tensor(0.9700))\n",
      "('s_0 obj features', tensor([[0.9700, 0.4500, 1.0000],\n",
      "        [0.0200, 0.9100, 1.0000]]))\n",
      "('s_1 obj features', tensor([[0.9700, 0.4500, 0.5500],\n",
      "        [0.0200, 0.9100, 0.5500]]))\n",
      "('reward', 0.97)\n"
     ]
    }
   ],
   "source": [
    "def Heuristic(Objects, scaling_factor=1 ):\n",
    "    \n",
    "    special_object = Objects[-1]\n",
    "    Objects.remove(special_object)\n",
    "    \n",
    "    Objects_sorted = sorted(Objects, key=lambda obj: -float(obj.Prize)/obj.Weight)\n",
    "    weight_total = 0\n",
    "    price_total = 0\n",
    "    for obj in Objects_sorted:\n",
    "        if weight_total+obj.Weight>1:\n",
    "            continue\n",
    "        price_total+=obj.Prize\n",
    "        weight_total+=obj.Weight\n",
    "    Objects.append(special_object)\n",
    "    \n",
    "    target = torch.tensor(price_total).float()\n",
    "    target = scaling_factor*target # sclaed target to have positive reward for good actions\n",
    "    target.requires_grad = False\n",
    "    \n",
    "    return target\n",
    "\n",
    "class StateClass:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    res_capacity = None\n",
    "    Chosen = None\n",
    "    price = None\n",
    "    Final = None\n",
    "    Selectable = None\n",
    "    SelectableObjectsFeatures = None\n",
    "    \n",
    "    def __init__(self, ObjectsFeatures, res_capacity, Chosen, price, env):\n",
    "        \n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        self.res_capacity = res_capacity\n",
    "        self.Chosen = Chosen\n",
    "        self.price = price\n",
    "        self.Final = 0\n",
    "        self.Selectable = self.StateMaskFunction(env)\n",
    "        if len(self.Selectable)==0:\n",
    "            self.Final=1\n",
    "        self.SelectableObjectsFeatures = self.SelectableObjectsFeaturesFunction()\n",
    "    \n",
    "        return\n",
    "\n",
    "    def StateMaskFunction(self, env):\n",
    "        \n",
    "        indexes = [i for i in range(len(env.Objects)) \n",
    "            if i not in self.Chosen and \n",
    "            not (self.res_capacity - env.Objects[i].Weight < 0)]\n",
    "    \n",
    "        return indexes \n",
    "    \n",
    "    def SelectableObjectsFeaturesFunction(self):\n",
    "        \n",
    "        SelectableObjectsFeatures = []\n",
    "        for i in range(len(self.ObjectsFeatures)):\n",
    "            if i in self.Selectable:\n",
    "                SelectableObjectsFeatures.append(self.ObjectsFeatures[i])\n",
    "        \n",
    "        if len(SelectableObjectsFeatures)>0:\n",
    "            SelectableObjectsFeatures = torch.stack(SelectableObjectsFeatures)\n",
    "        \n",
    "        return SelectableObjectsFeatures\n",
    "\n",
    "class Environment:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    Objects = None\n",
    "    capacity = None\n",
    "    target = None\n",
    "    \n",
    "    def __init__(self, num_objs, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, scaling_factor = 1, shuffle = False):\n",
    "        \n",
    "        if FakeBool:\n",
    "            # fake code to control the instance I fed it.\n",
    "            # best solution = [1,0,1,0,1,0,1,0,1,0,1]\n",
    "            Objects = []\n",
    "            for i in range(num_objs):\n",
    "                if i%2==1:\n",
    "                    prize = 0.1\n",
    "                    weight = 0.99\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "                else:\n",
    "                    prize = 1\n",
    "                    weight = (2/float(num_objs))-0.001\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "            if shuffle:\n",
    "                random.shuffle(Objects)\n",
    "            target = torch.tensor(int(num_objs/2)).float() # real target\n",
    "            target = scaling_factor*target # sclaed target to have positive reward for good actions\n",
    "            target.requires_grad = False\n",
    "            ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "            ObjectsFeatures.requires_grad = False            \n",
    "\n",
    "        else:\n",
    "            Objects = []\n",
    "            for i in range(num_objs):\n",
    "                prize = round(random.uniform(prize_min, prize_max), 2)\n",
    "                weight = round(random.uniform(weight_min, weight_max), 2)\n",
    "                obj = ObjectCass(prize,weight)\n",
    "                Objects.append(obj)\n",
    "            target = Heuristic(Objects, scaling_factor)\n",
    "            ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "            ObjectsFeatures.requires_grad = False\n",
    "\n",
    "        capacity = torch.tensor([1]).float() #\n",
    "        capacity.requires_grad = False\n",
    "\n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        self.Objects = Objects\n",
    "        self.capacity = capacity\n",
    "        self.target = target\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def CreateInitialState(self):\n",
    "        \n",
    "        price = torch.tensor(0).float() #\n",
    "        price.requires_grad = False\n",
    "        ObjectsFeatures = []\n",
    "        for obj in self.ObjectsFeatures:\n",
    "            ObjectsFeatures.append(torch.cat([obj, self.capacity]))\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures)\n",
    "        \n",
    "        return StateClass(ObjectsFeatures, self.capacity, [], price, self)\n",
    "\n",
    "    def step(self, state_old, obj_chosen):\n",
    "\n",
    "        capacity = state_old.res_capacity-state_old.ObjectsFeatures[obj_chosen][1].item()\n",
    "        price = state_old.price+state_old.ObjectsFeatures[obj_chosen][0].item()\n",
    "        New_Chosen = [old_chosen for old_chosen in state_old.Chosen]\n",
    "        New_Chosen.append(obj_chosen)\n",
    "        ObjectsFeatures = state_old.ObjectsFeatures.clone()\n",
    "        for i in range(len(ObjectsFeatures)):\n",
    "            ObjectsFeatures[i][-1] = capacity\n",
    "        state_new = StateClass(ObjectsFeatures, capacity, New_Chosen, price, self)\n",
    "        \n",
    "        # new state , reward\n",
    "        return state_new, env.Objects[obj_chosen].Prize\n",
    "    \n",
    "    def RandomPicker(self):\n",
    "        \n",
    "        Chosen = []\n",
    "        res_cap = 0\n",
    "        possible_objs = [obj for obj in self.Objects if obj.Weight + res_cap < self.capacity and obj not in Chosen]\n",
    "        price = 0\n",
    "        while len(possible_objs)>0:\n",
    "            obj = random.choice(possible_objs)\n",
    "            res_cap+=obj.Weight\n",
    "            price+=obj.Prize\n",
    "            Chosen.append(obj)\n",
    "            possible_objs = [obj for obj in self.Objects if obj.Weight + res_cap < self.capacity and obj not in Chosen]\n",
    "        \n",
    "        return price\n",
    "\n",
    "env = Environment(2)\n",
    "s_0 = env.CreateInitialState()\n",
    "s_1, reward = env.step(s_0,0)\n",
    "print('s_0 cap', s_0.res_capacity)\n",
    "print('s_1 cap', s_1.res_capacity)\n",
    "print('s_0 price', s_0.price)\n",
    "print('s_1 price', s_1.price)\n",
    "print('s_0 obj features', s_0.ObjectsFeatures)\n",
    "print('s_1 obj features', s_1.ObjectsFeatures)\n",
    "print('reward', reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation based on: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a NN which uses an attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs):    \n",
    "        \n",
    "        super(Net_Attention, self).__init__()\n",
    "        \n",
    "        self.emb = torch.nn.Linear(num_input_features,  h_model)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=h_model, nhead=num_head, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=p_dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "                \n",
    "        # FINAL LAYER\n",
    "        self.final_layer_0 = torch.nn.Linear(h_model,h_model)\n",
    "        self.final_layer_1 = torch.nn.Linear(h_model,1)\n",
    "\n",
    "    def forward(self, States, requires_grad = True):\n",
    "        \n",
    "        ObjectsFeatures = [state.SelectableObjectsFeatures for state in States]\n",
    "        if requires_grad == False:\n",
    "            for obj in ObjectsFeatures:\n",
    "                if len(obj)==0:\n",
    "                    error\n",
    "        \n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures)\n",
    "        ObjectsFeatures.requires_grad = requires_grad # in case you don't need to compute the gradient\n",
    "        \n",
    "        # define relu operation\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        \n",
    "        E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "        # where \n",
    "        # S is the source sequence length,  \n",
    "        # N is the batch size,  \n",
    "        # E is the feature number        \n",
    "        # you want \n",
    "        # E.size() = S,N,E = num_obj x batch x h_model\n",
    "        \n",
    "        #c = E\n",
    "        c = self.transformer_encoder(E)\n",
    "        c = self.final_layer_0(c)\n",
    "        c = ReLU(c)\n",
    "        Qvals = self.final_layer_1(c)\n",
    "        Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "        Qvals = torch.transpose(Qvals,0,1)\n",
    "        \n",
    "        return Qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that, given a state, it returns the Q values. We have to define the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PrintMaxQvals(Qvals_print,  stringa = ''):\n",
    "    \n",
    "    print('printing just 100 points for readibility reasons')\n",
    "    plt.figure()\n",
    "    plt.plot(Qvals_print, label='Qmax '+ stringa)\n",
    "    plt.ylabel('Qvals ' + stringa)\n",
    "    plt.legend()\n",
    "    return\n",
    "\n",
    "def MaskFunctionArray(States, Environments, Qvalss):\n",
    "    \n",
    "    QI = [MaskFunction(States[i], Environments[i], Qvalss[i]) for i in range(len(States))]\n",
    "    Qvalss_mask = [q[0] for q in QI]\n",
    "    indexess = [q[1] for q in QI]\n",
    "    \n",
    "    #Qvalss_mask = []\n",
    "    #indexess = []\n",
    "    #for i in range(len(States)):\n",
    "    #    q,i = MaskFunction(States[i], Environments[i], Qvalss[i])\n",
    "    #    Qvalss_mask.append(q)\n",
    "    #    indexess.append(i)\n",
    "        \n",
    "    return Qvalss_mask, indexess\n",
    "\n",
    "def MaskFunction(state, env, Qvals):\n",
    "        \n",
    "    indexes = [i for i in range(len(env.Objects)) \n",
    "               if i not in state.Chosen and \n",
    "               not (state.res_capacity - env.Objects[i].Weight < 0)]\n",
    "    Qvals_out = [Qvals[i] for i in indexes]\n",
    "    if len(Qvals_out)!=0:\n",
    "        Qvals_out = torch.stack(Qvals_out)\n",
    "    \n",
    "    return Qvals_out, indexes\n",
    "\n",
    "class BufferClass:\n",
    "    \n",
    "    replay_lenght = None\n",
    "    minibatch_size = None\n",
    "    Buffer = None\n",
    "    \n",
    "    def __init__(self,replay_lenght, minibatch_size):\n",
    "        \n",
    "        self.replay_lenght = replay_lenght\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.Buffer = []\n",
    "        \n",
    "    def Add(self, Transitions):\n",
    "                \n",
    "        while len(self.Buffer)+len(Transitions)>=self.replay_lenght:\n",
    "            self.Buffer.remove(random.choice(self.Buffer))\n",
    "        for t in Transitions:\n",
    "            self.Buffer.append(t)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def Minibatch(self, new_transactions = 0):\n",
    "        \n",
    "        if len(self.Buffer) <= self.minibatch_size:\n",
    "            Elements = self.Buffer\n",
    "        else:\n",
    "            new_transactions = min(new_transactions, self.minibatch_size)\n",
    "            last_transactions = self.Buffer[-new_transactions:]\n",
    "            Elements = random.sample(self.Buffer, self.minibatch_size - new_transactions)\n",
    "            # here it could pick the smae transition twice!! Not a big deal but good to know\n",
    "            Elements = last_transactions + Elements\n",
    "            Minibatch = Elements\n",
    "\n",
    "        #States = [experience[0] for experience in Elements]\n",
    "        #Actions = [experience[1] for experience in Elements]\n",
    "        #Rewards = [experience[2] for experience in Elements]\n",
    "        #New_States = [experience[3] for experience in Elements]\n",
    "        #\n",
    "        #Minibatch = [States, Actions, Rewards, New_States]\n",
    "        \n",
    "        return Minibatch\n",
    "\n",
    "def Learning_step(Net_predict,Net_target,optimizer, Minibatch, gamma = 1):\n",
    "    \n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Losses = []\n",
    "    for experience in Minibatch:\n",
    "        state, action, reward, new_state = experience\n",
    "        action_index = state.Selectable.index(action)\n",
    "        if not new_state.Final==1:\n",
    "            loss = (reward + gamma*torch.max(Net_target([new_state], requires_grad = False).squeeze(dim = 0)) - Net_predict([state]).squeeze(dim = 0)[action_index])**2\n",
    "        else:\n",
    "            loss = (reward - Net_predict([state]).squeeze(dim = 0)[action_index])**2\n",
    "        Losses.append(loss)\n",
    "    Losses = torch.stack(Losses)\n",
    "    \n",
    "    #States = []\n",
    "    #Actions = []\n",
    "    #Rewards = []\n",
    "    #New_States = []\n",
    "    #for experience in Minibatch:\n",
    "    #    state, action, reward, new_state = experience\n",
    "    #    States.append(state)\n",
    "    #    Actions.append(action)\n",
    "    #    Rewards.append(reward)\n",
    "    #    New_States.append(new_state)    \n",
    "    #New_States_Target = [New_States[i] for i in range(len(New_States)) if New_States[i].Final==0]\n",
    "    #Max_New_States_Target, _ = torch.max(Net_target(New_States_Target, requires_grad = False), dim = 1)\n",
    "    #Max_New_States = []\n",
    "    #j = 0\n",
    "    #for i in range(len(New_States)):\n",
    "    #    if New_States[i].Final==0: # not final\n",
    "    #        Max_New_States.append(Max_New_States_Target[j])\n",
    "    #        j+=1\n",
    "    #    else:\n",
    "    #        Max_New_States.append(torch.tensor([0]))\n",
    "    #Rewards = torch.tensor(Rewards)\n",
    "    #Auxiliary = Net_predict(States)\n",
    "    #Predictions_Actions = [Auxiliary[i][Actions[i]] for i in range(len(Auxiliary))]\n",
    "    #Predictions_Actions = torch.stack(Predictions_Actions)\n",
    "    #stoptocheck\n",
    "    # note that some Max_New_States are zero.\n",
    "    #Losses = (Rewards + gamma*Max_New_States - Predictions_Actions)**2 \n",
    "    \n",
    "    loss = Losses.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping\n",
    "    clip_value = 1\n",
    "    for p in Net_predict.parameters():\n",
    "        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "    \n",
    "    # apply gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.tolist()\n",
    "\n",
    "def Evaluate(num_epoch, Net_predict, num_objs, FakeBool = False, shuffle = False, times = 1, images = True, examples = 0):\n",
    "    \n",
    "        \n",
    "    better = 0\n",
    "    for time_counter in range(1,times+1):\n",
    "        print('evaluating model..('+str(time_counter)+' out of '+str(times)+')')\n",
    "        # set to train\n",
    "        Net_predict.eval()\n",
    "        # define the number of epochs\n",
    "        T = []\n",
    "        R = []\n",
    "        R_random = []\n",
    "        for e_counter in range(num_epoch):\n",
    "            env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffle)\n",
    "            state = env.CreateInitialState()\n",
    "            while True:\n",
    "                Qvals = Net_predict([state])\n",
    "                Qvals = torch.squeeze(Qvals, dim = 0)\n",
    "                action = torch.argmax(Qvals)\n",
    "                action_index = state.Selectable[action]\n",
    "                new_state, reward = env.step(state, action_index)\n",
    "                state = new_state\n",
    "                # break if no more possible actions\n",
    "                if state.Final==1:\n",
    "                    break\n",
    "\n",
    "            T.append(env.target)\n",
    "            R.append(new_state.price)\n",
    "            R_random.append(env.RandomPicker())\n",
    "\n",
    "        R_norm = [R[i]/T[i] for i in range(len(R))]\n",
    "        R_random_norm = [R_random[i]/T[i] for i in range(len(R_random))]\n",
    "        T_norm = [1 for i in range(len(R))]\n",
    "        \n",
    "        if images:\n",
    "            plt.figure()\n",
    "            plt.plot(R)\n",
    "            plt.plot(R, label='Rewards')\n",
    "            plt.plot(R_random)\n",
    "            plt.plot(R_random, label='Rewards (random picker)')\n",
    "            plt.plot(T)\n",
    "            plt.plot(T, label='baseline')\n",
    "            plt.ylabel(' Rewards ' )\n",
    "            plt.legend()\n",
    "            plt.ylim(bottom=0)\n",
    "            plt.figure()\n",
    "            plt.plot(R_norm)\n",
    "            plt.plot(R_norm, label='Rewards (normalized)')\n",
    "            plt.plot(R_random_norm)\n",
    "            plt.plot(R_random_norm, label='Rewards (random picker) normalized')\n",
    "            plt.plot(T_norm)\n",
    "            plt.plot(T_norm, label='baseline (normalized)')\n",
    "            plt.ylabel(' Rewards (normalized)' )\n",
    "            plt.legend()\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "        print('average norm reward (net)   : ', np.mean(R_norm))\n",
    "        print('average norm reward (random): ', np.mean(R_random_norm))\n",
    "        if np.mean(R_norm)>np.mean(R_random_norm):\n",
    "            better+=1\n",
    "    print('\\n\\n\\n')\n",
    "    print('the NN was better than the random pikcer '+str(better)+' times out of '+str(times))            \n",
    "    print('\\n\\n\\nExample:')\n",
    "    if examples>0:\n",
    "        for example_counter in range(examples):\n",
    "            reward_example = 0\n",
    "            env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffle)\n",
    "            print('Objects: ')\n",
    "            for i in range(len(env.Objects)):\n",
    "                print('object num '+str(i)+' price: '+str(env.Objects[i].Prize)+' weight: '+str(env.Objects[i].Weight))\n",
    "            print('heuristic target: ', env.target)\n",
    "            state = env.CreateInitialState()\n",
    "            while True:\n",
    "                Qvals = Net_predict([state])\n",
    "                Qvals = torch.squeeze(Qvals, dim = 0)\n",
    "                action = torch.argmax(Qvals)\n",
    "                action_index = state.Selectable[action]\n",
    "                print('NN selects: ', action_index)\n",
    "                new_state, reward = env.step(state, action_index)\n",
    "                state = new_state\n",
    "                reward_example+= env.Objects[action_index].Prize\n",
    "                # break if no more possible actions\n",
    "                if state.Final==1:\n",
    "                    break\n",
    "            print('NN reward ', reward_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the whole RL framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_input_features = 3 # prize, weight, res_capacity \n",
    "num_objs = 10\n",
    "num_outputs = num_objs\n",
    "h_model = 4\n",
    "num_head = 2\n",
    "num_layers = 2\n",
    "dim_feedforward = 4\n",
    "p_dropout = 0.1\n",
    "\n",
    "Net_predict = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "Net_target = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fake Instance?\n",
    "FakeBool = False\n",
    "shuffleBool = True\n",
    "# define the number of epochs\n",
    "num_epoch = 2500\n",
    "# epsilon\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "#\n",
    "minibatch_size = 16\n",
    "# C (how often update Net_target)\n",
    "C = max(int(num_epoch/100),100)\n",
    "# unit (for printing)\n",
    "unit = max(int(num_epoch/100),1)\n",
    "PATH_predict = '/home/big_bamboo/Downloads/Net_Predict'\n",
    "PATH_target = '/home/big_bamboo/Downloads/Net_Target'\n",
    "\n",
    "# is it a new test?\n",
    "LoadTest = False\n",
    "if LoadTest:\n",
    "    Net_predict.load_state_dict(torch.load(PATH_predict))\n",
    "    Net_target.load_state_dict(torch.load(PATH_target))\n",
    "    with open('RLKP-num_epoch_start-Save.pkl', 'rb') as input:\n",
    "        num_epoch_start = pickle.load(input)\n",
    "    with open('RLKP-Replay-buffer-Save.pkl', 'rb') as input:\n",
    "        ReplayBuffer = pickle.load(input)\n",
    "    with open('RLKP-Qvals_print_init-Save.pkl', 'rb') as input:\n",
    "        Qvals_print_init = pickle.load(input)\n",
    "    with open('RLKP-Qvals_print_final-Save.pkl', 'rb') as input:\n",
    "        Qvals_print_final = pickle.load(input)\n",
    "    with open('RLKP-optimizer-Save.pkl', 'rb') as input:\n",
    "        optimizer = pickle.load(input)\n",
    "else:\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.SGD(Net_predict.parameters(), lr=1e-3, momentum = 0.9)\n",
    "    # replay buffer\n",
    "    replay_lenght = 1e6\n",
    "    ReplayBuffer = BufferClass(replay_lenght,minibatch_size)\n",
    "    Qvals_print_init = []\n",
    "    Qvals_print_final = []\n",
    "    num_epoch_start = 0\n",
    "\n",
    "def SaveModel(Net_predict,Net_target,ReplayBuffer, e_counter, Qvals_print_init, Qvals_print_final):\n",
    "    \n",
    "    torch.save(Net_predict.state_dict(), PATH_predict)\n",
    "    torch.save(Net_target.state_dict(), PATH_target)\n",
    "    with open('RLKP-num_epoch_start-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(e_counter, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Replay-buffer-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(ReplayBuffer, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Qvals_print_init-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Qvals_print_init, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-Qvals_print_final-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Qvals_print_final, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RLKP-optimizer-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(optimizer, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# optimizer scheduler\n",
    "lmbda = lambda epoch: 1\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EpsGreedy(Qvals,indexes, e_counter):\n",
    "    \n",
    "    # select action\n",
    "    epsilon = (epsilon_max-epsilon_min)*(num_epoch-e_counter)/num_epoch + epsilon_min\n",
    "    if random.random() < epsilon:\n",
    "        action_index = random.choice(indexes)\n",
    "    else:\n",
    "        action = torch.argmax(Qvals)\n",
    "        action_index = indexes[action]\n",
    "    \n",
    "    return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch: ', 0)\n",
      "('epoch: ', 100)\n",
      "('epoch: ', 200)\n",
      "('epoch: ', 300)\n",
      "('epoch: ', 400)\n",
      "('epoch: ', 500)\n",
      "('learning rate ', 0.001)\n",
      "('epoch: ', 600)\n",
      "('epoch: ', 700)\n",
      "('epoch: ', 800)\n",
      "('epoch: ', 900)\n",
      "('epoch: ', 1000)\n",
      "('learning rate ', 0.001)\n"
     ]
    }
   ],
   "source": [
    "# set to train\n",
    "Net_predict.train()\n",
    "Q_init_print = []\n",
    "Q_final_print = []\n",
    "\n",
    "for e_counter in range(num_epoch_start, num_epoch):\n",
    "    if (e_counter)%100==0:\n",
    "        print('epoch: ',(e_counter))\n",
    "        # save model\n",
    "        #SaveModel(Net_predict,Net_target,ReplayBuffer, e_counter, Qvals_print_init, Qvals_print_final)\n",
    "    # creating instance\n",
    "    env = Environment(num_objs, FakeBool = FakeBool, shuffle = shuffleBool)\n",
    "    state = env.CreateInitialState()\n",
    "    Transitions = []\n",
    "    first = True\n",
    "    while True:\n",
    "        # Compute Qvals\n",
    "        Qvals = Net_predict([state])\n",
    "        Qvals = Qvals.squeeze() # reduce dimension\n",
    "        # just for printing \n",
    "        if first:\n",
    "            first = False\n",
    "            if e_counter%max(int((num_epoch+minibatch_size)/100),1)==0:\n",
    "                Q_init_print.append(torch.max(Qvals))\n",
    "        # select action\n",
    "        action_index = EpsGreedy(Qvals,state.Selectable, e_counter)\n",
    "        # environment step\n",
    "        new_state, reward = env.step(state, action_index)\n",
    "        Transitions.append([state, action_index, reward, new_state])\n",
    "        state = new_state\n",
    "        # break if no more possible actions\n",
    "        if state.Final==1:\n",
    "            if e_counter%max(int((num_epoch+minibatch_size)/100),1)==0:\n",
    "                Q_final_print.append(torch.max(Qvals))\n",
    "            break\n",
    "    # replay buffer\n",
    "    ReplayBuffer.Add(Transitions)    \n",
    "    if (e_counter- minibatch_size)>= minibatch_size:\n",
    "        l = Learning_step(Net_predict,Net_target,optimizer, ReplayBuffer.Minibatch(new_transactions = len(Transitions)))\n",
    "    if (e_counter)%C == 0:\n",
    "        Net_target.load_state_dict(Net_predict.state_dict())\n",
    "    if (e_counter)%int(num_epoch/5)==0 and e_counter>minibatch_size:\n",
    "        scheduler.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate ', param_group['lr'])\n",
    "            if param_group['lr']<1e-10:\n",
    "                print('the lr dropped too low, something is wrong')\n",
    "                error\n",
    "\n",
    "PrintMaxQvals(Q_init_print,  stringa = 'initial Q')\n",
    "PrintMaxQvals(Q_final_print,  stringa = 'final Q')\n",
    "Evaluate(100, Net_predict, num_objs, FakeBool = FakeBool, shuffle = shuffleBool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Evaluate(100, Net_predict, num_objs, FakeBool = False, shuffle = shuffleBool, times = 10, images = False, examples = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to try your own instance? Here you go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set to train\n",
    "Net_predict.eval()\n",
    "Net_target.eval()\n",
    "env = Environment(num_objs)\n",
    "# first prize, second weight\n",
    "Objects_numbers = [\n",
    "    [0.98, 0.23], # obj 0\n",
    "    [0.76, 0.45], # obj 1\n",
    "    [0.54, 0.67], # obj 2\n",
    "    [0.32, 0.89], # obj 3\n",
    "    [0.10, 0.11], # obj 4\n",
    "    [0.12, 0.33], # obj 5\n",
    "    [0.34, 0.55], # obj 6\n",
    "    [0.56, 0.77], # obj 7\n",
    "    [0.78, 0.99], # obj 8\n",
    "    [0.99, 0.22] # obj 9\n",
    "]\n",
    "\n",
    "Objects = []\n",
    "for i in range(len(Objects_numbers)):\n",
    "    prize = Objects_numbers[i][0]\n",
    "    weight = Objects_numbers[i][1]\n",
    "    obj = ObjectCass(prize,weight)\n",
    "    Objects.append(obj)\n",
    "env.Objects = Objects\n",
    "env.target = Heuristic(Objects, 1)\n",
    "env.ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "env.ObjectsFeatures.requires_grad = False\n",
    "print('heuristic taget: ', env.target)\n",
    "state = env.CreateInitialState()\n",
    "total_reward = 0\n",
    "while True:\n",
    "    # Compute Qvals\n",
    "    Qvals = Net_predict([state])\n",
    "    Qvals = Qvals.squeeze() # reduce dimension\n",
    "    # select action\n",
    "    action = torch.argmax(Qvals)\n",
    "    action_index = state.Selectable[action]\n",
    "    print('NN chooses obj: ', action_index)\n",
    "    # environment step\n",
    "    new_state, reward = env.step(state, action_index)\n",
    "    total_reward+=reward\n",
    "    state = new_state\n",
    "    # break if no more possible actions\n",
    "    if state.Final==1:\n",
    "        break\n",
    "print('NN reward: ', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- if you solve with FakeBool = True and then with FakeBool = False (both with 250 iterations), it achieves very good solutions. Why?? (Most likely because you re-used the same NN of before and the fake instances are very informative)\n",
    "\n",
    "STILL TO  DO:\n",
    "- investigate why it is still so slow the training (compare it with the evaluation)\n",
    "- find a way to make minibatch_size = 1024 without making everything crash? -- Are we sure about this?\n",
    "- compute losses in (mini)batches -> this does not work if you feed only the relevant objects because you can't stack them (they all have different dimensions)\n",
    "- create environment in batches (same as previous point)\n",
    "\n",
    "DONE:\n",
    "- make sure that the newest sequences are in the minibatch #DONE\n",
    "- put res_capacity as a repeated object feature in the state (now it is disregarded) # DONE\n",
    "- feed the NN just the releveant objects # DONE but this forbids to batch the creation of new environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
