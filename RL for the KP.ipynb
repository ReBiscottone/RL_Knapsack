{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all!\n",
    "In this script, we use (deep) reinforcement learning to solve the knapsack problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the knapsack problem (KP)? \n",
    "The KP is a problem where you are given a knapsack (a bag) with limited capacity and a set of objets.\n",
    "Objects have two attributes, prize and weight. Your goal is to select some objects such that you maximize the prize of the object chosen without exceeding the capacity of the knapsack.\n",
    "\n",
    "For example, given a knapsack of capacity 1 (also in the following, we assume weights to be normalized, so capacity knapsack = 1) and objects [prize, weight] = {[3,0.8],[1,0.25],[1,0.25],[1,0.25],[1,0.25]} the best solution would be to NOT pick up $obj_{0}$ (even if it has the greatest prize) while picking up all the other objects.\n",
    "\n",
    "Why I've chosen the KP? Because the KP is often considered the easiet among the NP-Hard problem (meaning that it takes exponential number of steps to achieve the optimal solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingredients of out RL will be the following:\n",
    "- multi task RL (because every instance is different)\n",
    "- DQN: deep Q-network\n",
    "- value approxiamtion via deep NN. Since the solution does not depends on the order of the objects (and for many other reasons), we use the attention mechanism in the NN. Attention is permutation - indifferent.\n",
    "- off policy. We use eps-greedy to explore but we use greedy to test.\n",
    "\n",
    "The overall script will be devided in (more or less) five big modules for repetibility usage.\n",
    "1. environment (generate tasks, new states, rewards,..)\n",
    "2. replay memory (memorizes transitions)\n",
    "3. Model (decides the exploration/evaluation policy)\n",
    "4. Learner (updates the model. input: memory, model)\n",
    "5. Runner (runs episodes. input: environment, model)\n",
    "\n",
    "a sixth module for printing is present, but that does not affect the overall algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "import time \n",
    "from collections import deque # for the buffer\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I still doesnt work I dont get why I cant enable notebook extensions\n",
    "# make sure you have the pep8_magic installed\n",
    "# jupyter nbextension install --user pep8_magic.py\n",
    "#%load_ext pep8_magic\n",
    "#%pep8\n",
    "#a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the first big module: ENVIRONMENT\n",
    "- the environment generates a task with num_objs (input) objects. \n",
    "- the enviroment can either generate a real instance (by default) or a fake one of which we know the optimal solution.\n",
    "- the environment returns the objects and their target (which is computed via an heuristic). # CHANGE THIS\n",
    "\n",
    "NOTE! for some aspects, what we call environment is actually more a task than an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('s_0 cap', tensor(1.))\n",
      "('s_1 cap', tensor(0.5500))\n",
      "('s_0 total reward', tensor(0.))\n",
      "('s_1 total reward', tensor(0.9700))\n",
      "('s_0 obj features', tensor([[0.9700, 0.4500, 1.0000, 1.0000],\n",
      "        [0.0200, 0.9100, 1.0000, 1.0000]]))\n",
      "('s_1 obj features', tensor([[0.9700, 0.4500, 0.0000, 0.5500],\n",
      "        [0.0200, 0.9100, 0.0000, 0.5500]]))\n",
      "('reward', tensor(0.9700))\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([0, 1])\n",
      "tensor([0])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([0, 1])\n",
      "tensor([], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "class ObjectCass:\n",
    "    \n",
    "    Prize = None\n",
    "    Weight = None\n",
    "    \n",
    "    def __init__(self, reward, weight):\n",
    "        \n",
    "        self.Prize = reward\n",
    "        self.Weight = weight\n",
    "\n",
    "class StateClass:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    res_capacity = None\n",
    "    Chosen = None\n",
    "    price = None\n",
    "    Final = None\n",
    "    IndexesActionsNotMasked = None\n",
    "    \n",
    "    def __init__(self, ObjectsFeatures, res_capacity, Chosen, price, Task):\n",
    "        \n",
    "        self.res_capacity = res_capacity\n",
    "        self.Chosen = Chosen\n",
    "        self.price = price\n",
    "        self.Final = 0\n",
    "        self.IndexesActionsNotMasked = self.StateMaskFunction(Task)\n",
    "        if len(self.IndexesActionsNotMasked)==0:\n",
    "            self.Final=1\n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        # change the boolean feature of the not selectable actions\n",
    "\n",
    "        for i in range(len(self.ObjectsFeatures)):\n",
    "            if i not in self.IndexesActionsNotMasked:\n",
    "                self.ObjectsFeatures[i][2]=0\n",
    "\n",
    "        return\n",
    "\n",
    "    def StateMaskFunction(self, Task):\n",
    "        \n",
    "        indexes = torch.Tensor([i for i in range(len(Task.Objects)) \n",
    "                    if i not in self.Chosen and \n",
    "                    not (self.res_capacity - Task.Objects[i].Weight < 0)]).long()\n",
    "\n",
    "        return indexes \n",
    "    \n",
    "class TaskClass:\n",
    "    \n",
    "    Objects = None\n",
    "    total_capacity = None\n",
    "    target = None\n",
    "    \n",
    "    def __init__(self, Objects, total_capacity):\n",
    "        \n",
    "        self.Objects = Objects\n",
    "        self.total_capacity = total_capacity\n",
    "        self.target = self.Heuristic(Objects)\n",
    "\n",
    "    def Heuristic(self, Objects):\n",
    "    \n",
    "        Objects_sorted = sorted(Objects, key=lambda obj: -float(obj.Prize)/obj.Weight)\n",
    "        weight_total = torch.tensor(0).float()\n",
    "        price_total  = torch.tensor(0).float()\n",
    "        for obj in Objects_sorted:\n",
    "            if weight_total+obj.Weight>1:\n",
    "                continue\n",
    "            price_total+=obj.Prize\n",
    "            weight_total+=obj.Weight\n",
    "\n",
    "        return price_total\n",
    "    \n",
    "    def initial_state(self):\n",
    "        \n",
    "        price   = torch.tensor(0).float() #\n",
    "        boolean = torch.tensor(1).float()\n",
    "        \n",
    "        ObjectsFeatures = []\n",
    "        for i in range(len(self.Objects)):\n",
    "            ObjectsFeatures.append(torch.stack([self.Objects[i].Prize, self.Objects[i].Weight, boolean , self.total_capacity]))\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures) \n",
    "        \n",
    "        return StateClass(ObjectsFeatures, self.total_capacity, torch.tensor([], dtype=torch.long), price, self)\n",
    "    \n",
    "    def step(self, state_old, action):\n",
    "    \n",
    "        # selecting object weight \n",
    "        # the trick is to first isolate the row related to the action\n",
    "        # then to isolate the element number torch.tensor(1) which is the weight\n",
    "        # the zero in fuction index_select are the dimensions,\n",
    "        # which is basically how you read the tensor\n",
    "        obj_features = state_old.ObjectsFeatures.index_select(0, action).squeeze()\n",
    "        obj_weight = obj_features.index_select(0, torch.tensor(1)).squeeze()\n",
    "        obj_price = obj_features.index_select(0, torch.tensor(0)).squeeze()\n",
    "        capacity = state_old.res_capacity-obj_weight\n",
    "        price = state_old.price+obj_price\n",
    "        New_Chosen = state_old.Chosen.clone()\n",
    "        action = action.long()\n",
    "        New_Chosen = torch.cat((\n",
    "            New_Chosen, \n",
    "            action.unsqueeze(dim=0)\n",
    "                               ))\n",
    "        ObjectsFeatures = state_old.ObjectsFeatures.clone()\n",
    "        \n",
    "        for i in range(len(ObjectsFeatures)):\n",
    "            ObjectsFeatures[i][-1] = capacity\n",
    "        # the boolens of the unchosable objects will be set to zero with the following command\n",
    "        state_new = StateClass(ObjectsFeatures, capacity, New_Chosen, price, self)\n",
    "        \n",
    "        # new state , reward\n",
    "        return state_new, self.Objects[action].Prize\n",
    "\n",
    "class EnvironmentClass:\n",
    "    \n",
    "    num_objs = None\n",
    "    \n",
    "    def __init__(self, num_objs):\n",
    "        \n",
    "        self.num_objs = num_objs\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def NewTask(self, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False):\n",
    "  \n",
    "        if FakeBool:\n",
    "            # fake task\n",
    "            # best solution = [1,0,1,0,1,0,1,0,1,0,1]\n",
    "            Objects = []\n",
    "            for i in range(self.num_objs):\n",
    "                if i%2==1:\n",
    "                    prize = 0.1\n",
    "                    weight = 0.99\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "                else:\n",
    "                    prize = 1\n",
    "                    weight = (2/float(num_objs))-0.001\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "            if shuffle:\n",
    "                random.shuffle(Objects)\n",
    "\n",
    "        else:\n",
    "            Objects = []\n",
    "            for i in range(self.num_objs):\n",
    "                prize  = torch.tensor(round(random.uniform(prize_min, prize_max), 2))\n",
    "                weight = torch.tensor(round(random.uniform(weight_min, weight_max), 2))\n",
    "                obj = ObjectCass(prize,weight)\n",
    "                Objects.append(obj)\n",
    "\n",
    "        capacity = torch.tensor(1).float() #\n",
    "\n",
    "        self.Objects = Objects\n",
    "        self.capacity = capacity\n",
    "            \n",
    "        return TaskClass(Objects, capacity)   \n",
    "    \n",
    "    def InitialStateNewTask(self, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False):\n",
    "        \n",
    "        Task = self.NewTask(FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False)\n",
    "        initial_state = Task.initial_state()\n",
    "        \n",
    "        return initial_state, Task\n",
    "    \n",
    "#just trying if it works\n",
    "env = EnvironmentClass(2)\n",
    "s_0, task = env.InitialStateNewTask()\n",
    "s_1, reward = task.step(s_0,torch.tensor(0))\n",
    "print('s_0 cap', s_0.res_capacity)\n",
    "print('s_1 cap', s_1.res_capacity)\n",
    "print('s_0 total reward', s_0.price)\n",
    "print('s_1 total reward', s_1.price)\n",
    "print('s_0 obj features', s_0.ObjectsFeatures)\n",
    "print('s_1 obj features', s_1.ObjectsFeatures)\n",
    "print('reward', reward)\n",
    "s_2, reward = task.step(s_1,torch.tensor(1))\n",
    "print(s_0.Chosen)\n",
    "print(s_0.IndexesActionsNotMasked)\n",
    "print(s_1.Chosen)\n",
    "print(s_1.IndexesActionsNotMasked)\n",
    "print(s_2.Chosen)\n",
    "print(s_2.IndexesActionsNotMasked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second big module: Model\n",
    "- the model contains the training/evaluation policy and it is responsable for choosing the actions\n",
    "- it contains the NN used for value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs):    \n",
    "        \n",
    "        super(Net_Attention, self).__init__()\n",
    "        \n",
    "        # from object features to high-dimension object features\n",
    "        self.emb = torch.nn.Linear(num_input_features,  h_model)\n",
    "        # transformer (encoder)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=h_model, nhead=num_head, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=p_dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers, norm = torch.nn.LayerNorm(h_model)\n",
    "            )\n",
    "        #        \n",
    "        # FINAL LAYER\n",
    "        self.final_layer_0 = torch.nn.Linear(h_model,h_model)\n",
    "        self.final_layer_1 = torch.nn.Linear(h_model,1)\n",
    "\n",
    "    def forward(self, States, no_grad = False):\n",
    "        \n",
    "        #ObjectsFeatures = [state.ObjectsFeatures for state in States]\n",
    "        ObjectsFeatures = torch.stack([state.ObjectsFeatures for state in States])\n",
    "        \n",
    "        # define relu operation\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        \n",
    "        if no_grad: #doesn't compute the gradients (hence no back propagation, but faster). \n",
    "            with torch.no_grad(): # to be used only in evaluations and target (not in training the prediction)        \n",
    "                E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "                # where \n",
    "                # S is the source sequence length,  \n",
    "                # N is the batch size,  \n",
    "                # E is the feature number        \n",
    "                # you want \n",
    "                # E.size() = S,N,E = num_obj x batch x h_model\n",
    "                \n",
    "                c = self.transformer_encoder(E)\n",
    "                \n",
    "                c = self.final_layer_0(c)\n",
    "                c = ReLU(c)\n",
    "                Qvals = self.final_layer_1(c)\n",
    "                Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "                Qvals = torch.transpose(Qvals,0,1)\n",
    "                \n",
    "                return Qvals\n",
    "            \n",
    "        E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "        # where \n",
    "        # S is the source sequence length,  \n",
    "        # N is the batch size,  \n",
    "        # E is the feature number        \n",
    "        # you want \n",
    "        # E.size() = S,N,E = num_obj x batch x h_model\n",
    "        \n",
    "        c = self.transformer_encoder(E)\n",
    "                \n",
    "        c = self.final_layer_0(c)\n",
    "        c = ReLU(c)\n",
    "        Qvals = self.final_layer_1(c)\n",
    "        Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "        Qvals = torch.transpose(Qvals,0,1)\n",
    "        #print(Qvals.size())\n",
    "                \n",
    "        return Qvals\n",
    "\n",
    "class ModelClass():\n",
    "\n",
    "    def __init__(self, Net_current, Net_target, training_policy, evaluation_policy, epsilon_max, epsilon_min):\n",
    "        \n",
    "        self.Net_current = Net_current\n",
    "        self.Net_target  = Net_target\n",
    "        if training_policy!='eps-greedy' or evaluation_policy!='greedy':\n",
    "            print('at least one of this exploration/evaluation policy are still to implement')\n",
    "            error\n",
    "        else:\n",
    "            self.epsilon_min = epsilon_min\n",
    "            self.epsilon_max = epsilon_max\n",
    "        self.training_policy   = training_policy\n",
    "        self.evaluation_policy = evaluation_policy\n",
    "\n",
    "    def Greedy(self, Qvals, state):\n",
    "        \n",
    "        Qvals_selectable = [Qvals[i] for i in range(len(Qvals)) if i in state.IndexesActionsNotMasked]\n",
    "        Qvals_selectable = torch.stack(Qvals_selectable)\n",
    "        action = torch.argmax(Qvals_selectable)\n",
    "        action_index = state.IndexesActionsNotMasked[action]\n",
    "        \n",
    "        return action_index\n",
    "    \n",
    "    def EpsGreedy(self, Qvals,state, iteration_counter, num_iterations):\n",
    "    \n",
    "        # select action\n",
    "        epsilon = (epsilon_max-epsilon_min)*(num_iterations-iteration_counter)/num_iterations + epsilon_min\n",
    "        if random.random() < epsilon:\n",
    "            action_index = random.choice(state.IndexesActionsNotMasked)\n",
    "        else:\n",
    "            action_index = self.Greedy(Qvals,state)\n",
    " \n",
    "        return action_index\n",
    "    \n",
    "    def training_step(self, state, iteration_counter, num_iterations):\n",
    "\n",
    "        self.Net_current.train()\n",
    "        if self.training_policy == 'eps-greedy':\n",
    "            Qvals = self.Net_current([state])\n",
    "            Qvals = Qvals.squeeze()\n",
    "            return self.EpsGreedy(Qvals, state, iteration_counter, num_iterations), Qvals\n",
    "        error\n",
    "    \n",
    "    def evaluation_step(self, state):\n",
    "        \n",
    "        self.Net_current.eval()\n",
    "        if self.evaluation_policy == 'greedy':\n",
    "            Qvals = self.Net_current([state], no_grad = True)\n",
    "            Qvals = Qvals.squeeze()\n",
    "            return self.Greedy(Qvals, state)\n",
    "        error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that, given a state, it returns the Q values. We have to define the loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third module: BUFFER (aka replay memory)\n",
    "- the buffer contains the last #replay_lenght transictions.\n",
    "- each experience in the buffer is as a tuple <$s_{old}$, action, r, $s_{new}$>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemoryClass:\n",
    "    \n",
    "    replay_lenght = None\n",
    "    minibatch_size = None\n",
    "    Buffer = None\n",
    "    new_transitions = None\n",
    "    \n",
    "    def __init__(self,replay_lenght, minibatch_size):\n",
    "        \n",
    "        self.replay_lenght = replay_lenght\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.Buffer = []\n",
    "        self.new_transitions = 0\n",
    "        \n",
    "    def Add(self, Transitions):\n",
    "                \n",
    "        while len(self.Buffer)+len(Transitions)>=self.replay_lenght:\n",
    "            self.Buffer.remove(self.Buffer[0])\n",
    "        for t in Transitions:\n",
    "            self.Buffer.append(t)\n",
    "        self.new_transitions+=len(Transitions)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def Minibatch(self):\n",
    "        \n",
    "        if len(self.Buffer) <= self.minibatch_size:\n",
    "            Minibatch = self.Buffer\n",
    "            self.new_transitions = max(0, self.new_transitions-len(self.Buffer))\n",
    "        else:\n",
    "            if self.new_transitions == 0: # no priority\n",
    "                Minibatch = random.sample(self.Buffer, self.minibatch_size)\n",
    "            else:\n",
    "                new_transitions = min(self.new_transitions, self.minibatch_size)\n",
    "                Minibatch_new_transitions = [self.Buffer[i] \n",
    "                                             for i in range((len(self.Buffer)-new_transitions)\n",
    "                                                            ,(len(self.Buffer)))]\n",
    "                remaining_transitions = min(0, self.minibatch_size - new_transitions)\n",
    "                Minibatch_random = random.sample(self.Buffer, remaining_transitions)\n",
    "                Minibatch = Minibatch_new_transitions + Minibatch_random\n",
    "                self.new_transitions-=new_transitions\n",
    "                \n",
    "        # the idea here is to order the minibatch with first the experieces with final = 0\n",
    "        # and then the experiecnes with final = 1. This allows us in the learning_step\n",
    "        # to work on the two cases separately and then just concatenate the results\n",
    "        \n",
    "        Minibatch.sort(key=lambda experience: experience[-1].Final)\n",
    "        \n",
    "        # CHANGE THE WAY YOU SAVE THINFS IN THE BUFFER!!!!\n",
    "        # SAVE DIRECTLY AS THE LISTS BELOW\n",
    "                \n",
    "        States = [experience[0] for experience in Minibatch]\n",
    "        Actions = torch.stack([experience[1] for experience in Minibatch]).unsqueeze(dim=1)\n",
    "        Rewards = torch.stack([experience[2] for experience in Minibatch])\n",
    "        New_States = [experience[3] for experience in Minibatch]\n",
    "        \n",
    "        Minibatch = [States, Actions, Rewards, New_States]\n",
    "        \n",
    "        return Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth module: LEARNER\n",
    "- receives minibatches\n",
    "- updates the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LearnerClass():\n",
    "\n",
    "    Model = None\n",
    "    optimizer = None\n",
    "    gamma = None\n",
    "    tau = None\n",
    "    loss_method = None\n",
    "    \n",
    "    def __init__(self,Model, optimizer, tau, gamma = 1,loss_method = torch.nn.MSELoss(reduction='mean')):\n",
    "        \n",
    "        self.Model = Model\n",
    "        self.optimizer = optimizer\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.loss_method = loss_method\n",
    "\n",
    "    def Learning_step(self, Minibatch):\n",
    "                \n",
    "        # zero the gradient buffers\n",
    "        self.optimizer.zero_grad() \n",
    "        \n",
    "        # 'get' data from the minibatch\n",
    "        States, Actions, Rewards, New_States = Minibatch\n",
    "        \n",
    "        #\n",
    "        # computing Q_vals of the new state (we split for final and nonfinal states)\n",
    "        #\n",
    "        New_States_nonFinal = [New_States[i] for i in range(len(New_States)) if New_States[i].Final==0]\n",
    "        if len(New_States_nonFinal)>0:\n",
    "            _, indexes = torch.max(self.Model.Net_target(New_States_nonFinal, no_grad = True), dim = 1)\n",
    "            Max_New_States_nonFinal = self.Model.Net_current(New_States_nonFinal).gather(1, indexes.unsqueeze(dim = 1)).squeeze(dim = 1)\n",
    "        else:\n",
    "            Max_New_States_nonFinal = torch.tensor([])\n",
    "                \n",
    "        # here we take advantage of the fact that we ordered the minibatch with first all the nonfinal states\n",
    "        # and then the final states. So we define a zero vector for the final states and we concatenate it to\n",
    "        # the tensor of the Q_vals of the next state (for non final states)\n",
    "        \n",
    "        length_final_states = len(New_States)-len(New_States_nonFinal)\n",
    "        if length_final_states>0:\n",
    "            zeros = torch.zeros(length_final_states, dtype = torch.float)\n",
    "            Max_New_States = torch.cat((Max_New_States_nonFinal, zeros))\n",
    "        else: # all states are non final\n",
    "            Max_New_States = Max_New_States_nonFinal\n",
    "        \n",
    "        # get Qvals for the 'old' states\n",
    "        Predictions_Actions = self.Model.Net_current(States).gather(1, Actions).squeeze(dim = 1)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = self.loss_method(Predictions_Actions,Rewards + self.gamma*Max_New_States)\n",
    "        \n",
    "        # backpropagate loss\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        clip_value = 1\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.Model.Net_current.parameters(), clip_value)\n",
    "        \n",
    "        # apply gradient\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft target update\n",
    "        for param_target, param_current in zip(self.Model.Net_target.parameters(), self.Model.Net_current.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param_current.data * self.tau)        \n",
    "        \n",
    "        return loss.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth module: RUNNER\n",
    "- takes the environment and the module\n",
    "- runs a transaction. Or should it run an episode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunnerClass():\n",
    "\n",
    "    Environment = None\n",
    "    Model = None\n",
    "    \n",
    "    def __init__(self, Environment, Model):\n",
    "        \n",
    "        self.Model = Model\n",
    "        self.Environment = Environment\n",
    "        \n",
    "    def NewTrainingTask(self, iteration_counter,num_iterations):\n",
    "        \n",
    "        Transitions = []\n",
    "        state, task = self.Environment.InitialStateNewTask()\n",
    "        G = 0 # reward obtained\n",
    "        while state.Final == 0:\n",
    "            action_index, Q_vals = self.Model.training_step(state, iteration_counter,num_iterations)\n",
    "            # save initial_Q_vals (for printing reasons)\n",
    "            if G == 0: # it means this is the first action\n",
    "                initial_Q_vals = Q_vals\n",
    "            new_state, reward = task.step(state, action_index)\n",
    "            G+=reward\n",
    "            Transitions.append([state, action_index, reward, new_state])\n",
    "            state = new_state\n",
    "         \n",
    "        return Transitions, task.target, G, initial_Q_vals\n",
    "    \n",
    "    def NewEvaluationTask(self):\n",
    "        \n",
    "        state, task = self.Environment.InitialStateNewTask()\n",
    "        G = 0\n",
    "        while state.Final == 0:\n",
    "            action_index = self.Model.evaluation_step(state)\n",
    "            new_state, reward = task.step(state, action_index)\n",
    "            G+=reward\n",
    "            state = new_state\n",
    "         \n",
    "        return G, task.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function to evaluate the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Evaluate(num_iterations, Runner):\n",
    "    \n",
    "    better = 0\n",
    "    G_tot = []\n",
    "    heuristic_tot = []\n",
    "    for iteration_counter in range(num_iterations):\n",
    "        # perform new task\n",
    "        G, heuristic = Runner.NewEvaluationTask()\n",
    "        G_tot.append(G)\n",
    "        heuristic_tot.append(heuristic)\n",
    "        if G >= heuristic:\n",
    "            better+=1\n",
    "    print('the NN was better than (or equal to) the heuristic '+str(better)+' times out of '+str(num_iterations)) \n",
    "    \n",
    "    R_norm = [G_tot[i]/heuristic_tot[i] for i in range(len(G_tot))]\n",
    "    mean = np.mean(R_norm)\n",
    "    std = np.std(R_norm)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we also define a function to print stuff.\n",
    "(this function is not really smart, it recomputes the Q values. think of another way to do it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StuffToBePrintedClass():\n",
    "    \n",
    "    Initial_Qvals = None\n",
    "    Gs = None\n",
    "    heuristic_targets = None\n",
    "    PATH = None\n",
    "    Evaluation = None\n",
    "    loss = None\n",
    "\n",
    "    def __init__(self, path, Initial_Qvals=[], Gs=[], heuristic_targets=[], Evaluation=[], loss=[]):\n",
    "        \n",
    "        self.PATH = path\n",
    "        self.Initial_Qvals = Initial_Qvals\n",
    "        self.Gs = Gs\n",
    "        self.heuristic_targets = heuristic_targets\n",
    "        self.Evaluation = Evaluation\n",
    "        self.loss = loss\n",
    "    \n",
    "    def AddElements(self,Initial_Qvals='dummy', G='dummy', heuristic_target='dummy'\n",
    "                   , Evaluation='dummy', loss='dummy'):\n",
    "        \n",
    "        \"\"\" comment what it does.\n",
    "        \"\"\"\n",
    "        \n",
    "        if Initial_Qvals!='dummy':\n",
    "            self.Initial_Qvals.append(Initial_Qvals)\n",
    "        if G!='dummy':\n",
    "            self.Gs.append(G)\n",
    "        if heuristic_target!='dummy':\n",
    "            self.heuristic_targets.append(heuristic_target)\n",
    "        if Evaluation!='dummy':\n",
    "            self.Evaluation.append(Evaluation)\n",
    "        if loss!='dummy':\n",
    "            self.loss.append(loss)\n",
    "            \n",
    "        if G!='dummy' and Evaluation!='dummy':\n",
    "            error\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def Print(self):\n",
    "        \n",
    "        # Q vals\n",
    "        self.PrintSequence(self.Initial_Qvals, 'Initial Qvals', Q_valsBool = True)\n",
    "        Q_Actions_norm = []\n",
    "        for i in range(len(self.Initial_Qvals)):\n",
    "            Q_Action_i = [self.Initial_Qvals[i][j]/self.heuristic_targets[i] for j in range(len(self.Initial_Qvals[i]))]\n",
    "            Q_Actions_norm.append(Q_Action_i)\n",
    "        self.PrintSequence(Q_Actions_norm, 'Initial Qvals Normalized', Q_valsBool = True, Ones = True)\n",
    "        # non Q vals\n",
    "        self.PrintSequence(self.Gs, 'total rewards', y_min = 0, loc = 'lower right')\n",
    "        Gs_norm = [self.Gs[i]/self.heuristic_targets[i] for i in range(len(self.Gs))]\n",
    "        self.PrintSequence(Gs_norm, 'normalized total rewards', Ones = True, y_min = 0,loc = 'lower right')\n",
    "        self.PrintSequence(self.loss, 'loss', y_min = 0, loc = 'upper right')\n",
    "        # Evaluation\n",
    "        self.PrintEvaluation()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def PrintSequence(self, sequence, string, Q_valsBool = False, Ones = False, y_min = False, loc = 'dummy'):\n",
    "        \n",
    "        plt.figure()\n",
    "        x_axis = [i for i in range(len(sequence))]\n",
    "        if not Q_valsBool:\n",
    "            if len(sequence)>100:\n",
    "                means, stds, x_axis = self.MeanAndStandard(sequence) \n",
    "                #plt.errorbar(x_axis, means, yerr=stds, label=string) #, fmt='o' if you want the ball\n",
    "                y_low = [means[i]-stds[i] for i in range(len(means))]\n",
    "                y_upp = [means[i]+stds[i] for i in range(len(means))]\n",
    "                plt.fill_between(x_axis, y_low, y_upp, alpha=0.75,label=string)\n",
    "                #plt.plot(x_axis, means, label=string)\n",
    "            else:\n",
    "                plt.plot(sequence, label=string)\n",
    "        else:\n",
    "            if len(sequence)>100:\n",
    "                for i in range(len(sequence[0])):\n",
    "                    Q_Action_i = [sequence[j][i] for j in range(len(sequence))]\n",
    "                    means, stds, x_axis = self.MeanAndStandard(Q_Action_i)\n",
    "                    y_low = [means[i]-stds[i] for i in range(len(means))]\n",
    "                    y_upp = [means[i]+stds[i] for i in range(len(means))]\n",
    "                    plt.fill_between(x_axis, y_low, y_upp, alpha=0.2,label=string+'_act_'+str(i))\n",
    "                    #plt.errorbar(x_axis, means, yerr=stds, label=string+'_act_'+str(i)) # fmt='o'\n",
    "                    #plt.plot(x_axis, means, label=string+'_act_'+str(i))\n",
    "                Q_max = [max(Q_vals) for Q_vals in sequence]\n",
    "                means, stds, x_axis = self.MeanAndStandard(Q_max)                \n",
    "                y_low = [means[i]-stds[i] for i in range(len(means))]\n",
    "                y_upp = [means[i]+stds[i] for i in range(len(means))]\n",
    "                #plt.plot(x_axis, means, label='Q_Max')\n",
    "                #plt.errorbar(x_axis, means, yerr=stds, label='Q_Max') #, fmt='o'\n",
    "                plt.fill_between(x_axis, y_low, y_upp, alpha=0.2,label='Q_Max')\n",
    "            else:\n",
    "                for i in range(len(sequence[0])):\n",
    "                    Q_Action_i = [sequence[j][i] for j in range(len(sequence))]\n",
    "                    plt.plot(Q_Action_i, label=string+'_act_'+str(i))\n",
    "                Q_max = [max(Q_vals) for Q_vals in sequence]\n",
    "                plt.plot(Q_max, label='Q_Max')\n",
    "        if Ones:\n",
    "            Ones = [1 for j in range(len(x_axis))]\n",
    "            plt.plot(x_axis, Ones, label='normalized heuristic value')\n",
    "        if y_min:\n",
    "            plt.ylim(ymin=y_min)\n",
    "        plt.legend()\n",
    "        if loc!='dummy':\n",
    "            leg = plt.legend(loc=loc)\n",
    "        plt.savefig(self.PATH+string+'.png', bbox_inches='tight')\n",
    "        \n",
    "        # print a figure with just the maximum Q val (without the other Q vals)\n",
    "        if Q_valsBool:\n",
    "            plt.figure()\n",
    "            Q_max = [max(Q_vals) for Q_vals in sequence]\n",
    "            means, stds, x_axis = self.MeanAndStandard(Q_max)                \n",
    "            y_low = [means[i]-stds[i] for i in range(len(means))]\n",
    "            y_upp = [means[i]+stds[i] for i in range(len(means))]    \n",
    "            plt.fill_between(x_axis, y_low, y_upp, alpha=0.75,label='Q_Max')\n",
    "            if Ones:\n",
    "                Ones = [1 for j in range(len(x_axis))]\n",
    "                plt.plot(x_axis, Ones, label='normalized heuristic value')\n",
    "            plt.legend()\n",
    "            leg = plt.legend(loc='upper left')\n",
    "            plt.savefig(self.PATH+'Q_Max.png', bbox_inches='tight')\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def MeanAndStandard(self, sequence):                \n",
    "        \n",
    "        unit = int(len(sequence)/100)\n",
    "        Means = []\n",
    "        STD = []\n",
    "        Aux = []\n",
    "        for i in range(len(sequence)):\n",
    "            Aux.append(sequence[i])\n",
    "            if i%unit==0 and i!=0:\n",
    "                vector = np.array(Aux, dtype=float)\n",
    "                mean = np.mean(vector)\n",
    "                std = np.std(vector)\n",
    "                Means.append(mean)\n",
    "                STD.append(std)\n",
    "                Aux = []\n",
    "        x_axis = [i*unit for i in range(len(Means))]\n",
    "        \n",
    "        return Means, STD, x_axis\n",
    "\n",
    "    def PrintEvaluation(self):\n",
    "\n",
    "        plt.figure()\n",
    "        means  = [single_evaluation[0] for single_evaluation in self.Evaluation]\n",
    "        stds   = [single_evaluation[1] for single_evaluation in self.Evaluation]\n",
    "        x_axis = [single_evaluation[2] for single_evaluation in self.Evaluation]\n",
    "\n",
    "        plt.errorbar(x_axis, means, yerr=stds,fmt='o-', label = 'normalized rewards during evaluation') # \n",
    "        plt.ylim(bottom=0)\n",
    "        Ones = [1 for j in range(len(x_axis))]\n",
    "        plt.plot(x_axis, Ones, label = 'normalized heuristic rewards')\n",
    "        plt.legend()\n",
    "        leg = plt.legend( loc = 'lower right')\n",
    "        \n",
    "        plt.savefig(self.PATH+'Evaluation_Normalized_Rewards_per_iteration.png', bbox_inches='tight')\n",
    "\n",
    "def MaxInitialQvalues(Transitions, Net_predict):\n",
    "    \n",
    "    first_state, action_index, reward, new_state = Transitions[0]\n",
    "    Qvals = Net_predict([first_state], no_grad = True)\n",
    "    Qvals = Qvals.squeeze().tolist()\n",
    "    \n",
    "    return Qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains functions to save and load the data.\n",
    "The idea is to save data during training in PATH_AUX\n",
    "and save data after training in PATH_Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SaveData(iteration_counter,Learner,Runner,ReplayMemory,StuffToBePrinted, PATH_save):\n",
    "    \n",
    "    with open(PATH_save+'RLKP-num_iterations_start-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(iteration_counter, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-Learner-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Learner, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-Runner-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Runner, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-ReplayMemory-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(ReplayMemory, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-StuffToBePrinted-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(StuffToBePrinted, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return\n",
    "\n",
    "def LoadData(Path_load):\n",
    "    \n",
    "    with open(Path_load+'RLKP-num_iterations_start-Save.pkl', 'rb') as input:\n",
    "        num_iterations_start = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Learner-Save.pkl', 'rb') as input:\n",
    "        Learner = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Runner-Save.pkl', 'rb') as input:\n",
    "        Runner = pickle.load(input)\n",
    "    env = Runner.Environment\n",
    "    Model = Learner.Model\n",
    "    Runner.Model = Model\n",
    "    with open(Path_load+'RLKP-ReplayMemory-Save.pkl', 'rb') as input:\n",
    "        ReplayMemory = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-StuffToBePrinted-Save.pkl', 'rb') as input:\n",
    "        StuffToBePrinted = pickle.load(input)\n",
    "        \n",
    "    return num_iterations_start, Learner, Runner, env, Model, ReplayMemory, StuffToBePrinted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our 5 modules (or load them from existing ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# seed\n",
    "random.seed(1234)\n",
    "# num episode to be generated\n",
    "num_iterations = int(5e3)\n",
    "# num object per episode\n",
    "num_objs = 10\n",
    "num_input_features = 4 # prize, weight, res_capacity, Bool_alreday_chosen\n",
    "num_outputs = num_objs\n",
    "# transformer details\n",
    "h_model = 8\n",
    "num_head = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = h_model\n",
    "p_dropout = 0.1\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "# minimum and maximum epsilon value\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "# how many transiction can you store in the buffer\n",
    "replay_lenght = 1e6\n",
    "# minibatch dimension\n",
    "minibatch_size = 4\n",
    "# how many times do you want to evaluate the algorithm mid-traing?\n",
    "unit_evaluation = 10\n",
    "unit_evaluation = int(num_iterations/unit_evaluation)\n",
    "# soft update\n",
    "tau = 0.005\n",
    "# paths\n",
    "PATH = '/Users/BigBamboo/Desktop/RL for the KP/'\n",
    "PATH_Aux = PATH+'Saved_objects_Aux/'\n",
    "PATH_Final = PATH+'Final_Objects/'\n",
    "\n",
    "# is it a new test?\n",
    "LoadTest = False\n",
    "# objects in the middel of training (Aux) or completely computed (Final)\n",
    "Path_load = PATH_Aux\n",
    "if LoadTest:\n",
    "    num_iterations_start, Learner, Runner, env, Model, ReplayMemory, StuffToBePrinted = LoadData(Path_load)\n",
    "else:\n",
    "    # environment\n",
    "    env = EnvironmentClass(num_objs)\n",
    "    # model\n",
    "    Net_current = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "    Net_target  = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "    Model = ModelClass(Net_current, Net_target, 'eps-greedy', 'greedy', epsilon_max, epsilon_min)\n",
    "    # Replay Memory\n",
    "    ReplayMemory = ReplayMemoryClass(replay_lenght, minibatch_size)\n",
    "    # Learner\n",
    "    optimizer = torch.optim.RMSprop(Net_current.parameters(), lr = lr)\n",
    "    Learner = LearnerClass(Model, optimizer, tau)\n",
    "    # Runner\n",
    "    Runner = RunnerClass(env, Model)\n",
    "    # print\n",
    "    StuffToBePrinted = StuffToBePrintedClass(PATH_Final, [], [], [], [], [])    \n",
    "    # zero if you start a new test\n",
    "    num_iterations_start = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('task number: ', 0)\n",
      "the NN was better than (or equal to) the heuristic 0 times out of 100\n"
     ]
    }
   ],
   "source": [
    "for iteration_counter in range(num_iterations_start, num_iterations):\n",
    "    if iteration_counter%500==0:\n",
    "        SaveData(iteration_counter,Learner,Runner,ReplayMemory,StuffToBePrinted, PATH_Aux)\n",
    "        print('task number: ',(iteration_counter))\n",
    "    # perform new task\n",
    "    Transitions, target_heuristic, G, initial_Q_vals = Runner.NewTrainingTask(iteration_counter, num_iterations)\n",
    "    # save transition\n",
    "    ReplayMemory.Add(Transitions)\n",
    "    # learn        \n",
    "    if iteration_counter>= ReplayMemory.minibatch_size:\n",
    "        loss = Learner.Learning_step(ReplayMemory.Minibatch())\n",
    "        StuffToBePrinted.AddElements(Initial_Qvals = initial_Q_vals, G = G, \n",
    "                                     heuristic_target = target_heuristic, loss = loss)\n",
    "    # evaluate mid-training\n",
    "    if iteration_counter%unit_evaluation == 0:\n",
    "        mean, std = Evaluate(100, Runner)\n",
    "        StuffToBePrinted.AddElements(Evaluation = [mean, std, iteration_counter])\n",
    "\n",
    "# evaluate after training\n",
    "mean, std = Evaluate(100, Runner)\n",
    "StuffToBePrinted.AddElements(Evaluation = [mean, std, iteration_counter])\n",
    "# save data\n",
    "SaveData(num_iterations,Learner,Runner,ReplayMemory,StuffToBePrinted, PATH_Final)\n",
    "# print\n",
    "print('printing..')\n",
    "StuffToBePrinted.Print()\n",
    "print('.. end printing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# are you using the net from a previous test?\n",
    "LoadTest = True\n",
    "# objects in the middel of training (Aux) or completely computed (Final)\n",
    "Path_load = PATH_Aux\n",
    "Path_load = PATH_Final\n",
    "if LoadTest:\n",
    "    num_iterations_start, Learner, Runner, env, Model, ReplayMemory, StuffToBePrinted = LoadData(Path_load)\n",
    "mean, std = Evaluate(100, Runner)\n",
    "print('average normalized return; ', mean)\n",
    "print('with standard deviation  ; ', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- better heuristic / optimal solutions\n",
    "\n",
    "WRITING TIPS (https://www.python.org/dev/peps/pep-0008/#documentation-strings):\n",
    "- comment what the function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
