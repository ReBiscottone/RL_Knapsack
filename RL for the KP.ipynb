{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi all!\n",
    "In this script, we use (deep) reinforcement learning to solve the knapsack problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the knapsack problem (KP)? \n",
    "The KP is a problem where you are given a knapsack (a bag) with limited capacity and a set of objets.\n",
    "Objects have two attributes, prize and weight. Your goal is to select some objects such that you maximize the prize of the object chosen without exceeding the capacity of the knapsack.\n",
    "\n",
    "For example, given a knapsack of capacity 1 (also in the following, we assume weights to be normalized, so capacity knapsack = 1) and objects [prize, weight] = {[3,0.8],[1,0.25],[1,0.25],[1,0.25],[1,0.25]} the best solution would be to NOT pick up $obj_{0}$ (even if it has the greatest prize) while picking up all the other objects.\n",
    "\n",
    "Why I've chosen the KP? Because the KP is often considered the easiet among the NP-Hard problem (meaning that it takes exponential number of steps to achieve the optimal solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingredients of out RL will be the following:\n",
    "- multi task RL (because every instance is different)\n",
    "- DQN: deep Q-network\n",
    "- value approxiamtion via deep NN. Since the solution does not depends on the order of the objects (and for many other reasons), we use the attention mechanism in the NN. Attention is permutation - indifferent.\n",
    "- off policy. We use eps-greedy to explore but we use greedy to test.\n",
    "\n",
    "The overall script will be devided in (more or less) five big modules for repetibility usage.\n",
    "1. environment (generate tasks, new states, rewards,..)\n",
    "2. replay memory (memorizes transitions)\n",
    "3. Model (decides the exploration/evaluation policy)\n",
    "4. Learner (updates the model. input: memory, model)\n",
    "5. Runner (runs episodes. input: environment, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start by importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/big_bamboo/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the first big module: ENVIRONMENT\n",
    "- the environment generates a task with num_objs (input) objects. \n",
    "- the enviroment can either generate a real instance (by default) or a fake one of which we know the optimal solution.\n",
    "- the environment returns the objects and their target (which is computed via an heuristic). # CHANGE THIS\n",
    "\n",
    "NOTE! for some aspects, what we call environment is actually more a task than an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_0 cap tensor([1.])\n",
      "s_1 cap tensor([0.5500])\n",
      "s_0 price tensor(0.)\n",
      "s_1 price tensor(0.9700)\n",
      "s_0 obj features tensor([[0.9700, 0.4500, 1.0000, 1.0000],\n",
      "        [0.0200, 0.9100, 1.0000, 1.0000]])\n",
      "s_1 obj features tensor([[0.9700, 0.4500, 0.0000, 0.5500],\n",
      "        [0.0200, 0.9100, 0.0000, 0.5500]])\n",
      "reward 0.97\n"
     ]
    }
   ],
   "source": [
    "class ObjectCass:\n",
    "    \n",
    "    Prize = None\n",
    "    Weight = None\n",
    "    \n",
    "    def __init__(self, reward, weight):\n",
    "        \n",
    "        self.Prize = reward\n",
    "        self.Weight = weight\n",
    "\n",
    "class StateClass:\n",
    "    \n",
    "    ObjectsFeatures = None\n",
    "    res_capacity = None\n",
    "    Chosen = None\n",
    "    price = None\n",
    "    Final = None\n",
    "    IndexesActionsNotMasked = None\n",
    "    \n",
    "    def __init__(self, ObjectsFeatures, res_capacity, Chosen, price, Task):\n",
    "        \n",
    "        self.res_capacity = res_capacity\n",
    "        self.Chosen = Chosen\n",
    "        self.price = price\n",
    "        self.Final = 0\n",
    "        self.IndexesActionsNotMasked = self.StateMaskFunction(Task)\n",
    "        if len(self.IndexesActionsNotMasked)==0:\n",
    "            self.Final=1\n",
    "        self.ObjectsFeatures = ObjectsFeatures\n",
    "        # change the boolean feature of the not selectable actions\n",
    "        for i in range(len(self.ObjectsFeatures)):\n",
    "            if i not in self.IndexesActionsNotMasked:\n",
    "                self.ObjectsFeatures[i][2]=0.0\n",
    "        \n",
    "        return\n",
    "\n",
    "    def StateMaskFunction(self, Task):\n",
    "        \n",
    "        indexes = [i for i in range(len(Task.Objects)) \n",
    "                    if i not in self.Chosen and \n",
    "                    not (self.res_capacity - Task.Objects[i].Weight < 0)]\n",
    "    \n",
    "        return indexes \n",
    "    \n",
    "class TaskClass:\n",
    "    \n",
    "    Objects = None\n",
    "    total_capacity = None\n",
    "    target = None\n",
    "    \n",
    "    def __init__(self, Objects, total_capacity):\n",
    "        \n",
    "        self.Objects = Objects\n",
    "        self.total_capacity = total_capacity\n",
    "        self.target = self.Heuristic(Objects)\n",
    "\n",
    "    def Heuristic(self, Objects):\n",
    "    \n",
    "        Objects_sorted = sorted(Objects, key=lambda obj: -float(obj.Prize)/obj.Weight)\n",
    "        weight_total = 0\n",
    "        price_total = 0\n",
    "        for obj in Objects_sorted:\n",
    "            if weight_total+obj.Weight>1:\n",
    "                continue\n",
    "            price_total+=obj.Prize\n",
    "            weight_total+=obj.Weight\n",
    "\n",
    "        target = torch.tensor(price_total).float()\n",
    "        target.requires_grad = False\n",
    "\n",
    "        return target\n",
    "    \n",
    "    def initial_state(self):\n",
    "        \n",
    "        price = torch.tensor(0).float() #\n",
    "        \n",
    "        ObjectsFeatures = []\n",
    "        for i in range(len(self.Objects)):\n",
    "            ObjectsFeatures.append(torch.tensor([self.Objects[i].Prize, self.Objects[i].Weight, 1.0 , self.total_capacity]))\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures) \n",
    "        \n",
    "        return StateClass(ObjectsFeatures, self.total_capacity, [], price, self)\n",
    "    \n",
    "    def step(self, state_old, action):\n",
    "    \n",
    "        capacity = state_old.res_capacity-self.Objects[action].Weight\n",
    "        price = state_old.price+self.Objects[action].Prize\n",
    "        New_Chosen = copy.copy(state_old.Chosen)\n",
    "        New_Chosen.append(action)\n",
    "        ObjectsFeatures = state_old.ObjectsFeatures.clone()\n",
    "        for i in range(len(ObjectsFeatures)):\n",
    "            ObjectsFeatures[i][-1] = capacity\n",
    "        # the boolens of the unchosable objects will be set to zero with the following command\n",
    "        state_new = StateClass(ObjectsFeatures, capacity, New_Chosen, price, self)\n",
    "        \n",
    "        # new state , reward\n",
    "        return state_new, self.Objects[action].Prize\n",
    "\n",
    "class EnvironmentClass:\n",
    "    \n",
    "    num_objs = None\n",
    "    \n",
    "    def __init__(self, num_objs):\n",
    "        \n",
    "        self.num_objs = num_objs\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def NewTask(self, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False):\n",
    "  \n",
    "        if FakeBool:\n",
    "            # fake task\n",
    "            # best solution = [1,0,1,0,1,0,1,0,1,0,1]\n",
    "            Objects = []\n",
    "            for i in range(self.num_objs):\n",
    "                if i%2==1:\n",
    "                    prize = 0.1\n",
    "                    weight = 0.99\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "                else:\n",
    "                    prize = 1\n",
    "                    weight = (2/float(num_objs))-0.001\n",
    "                    obj = ObjectCass(prize,weight)\n",
    "                    Objects.append(obj)\n",
    "            if shuffle:\n",
    "                random.shuffle(Objects)\n",
    "\n",
    "        else:\n",
    "            Objects = []\n",
    "            for i in range(self.num_objs):\n",
    "                prize = round(random.uniform(prize_min, prize_max), 2)\n",
    "                weight = round(random.uniform(weight_min, weight_max), 2)\n",
    "                obj = ObjectCass(prize,weight)\n",
    "                Objects.append(obj)\n",
    "\n",
    "        capacity = torch.tensor([1]).float() #\n",
    "\n",
    "        self.Objects = Objects\n",
    "        self.capacity = capacity\n",
    "            \n",
    "        return TaskClass(Objects, capacity)   \n",
    "    \n",
    "    def InitialStateNewTask(self, FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False):\n",
    "        \n",
    "        Task = self.NewTask(FakeBool = False, prize_min=0.01, prize_max=1, weight_min=0.01, weight_max=1, shuffle = False)\n",
    "        initial_state = Task.initial_state()\n",
    "        \n",
    "        return initial_state, Task\n",
    "    \n",
    "\n",
    "env = EnvironmentClass(2)\n",
    "s_0, task = env.InitialStateNewTask()\n",
    "s_1, reward = task.step(s_0,0)\n",
    "print('s_0 cap', s_0.res_capacity)\n",
    "print('s_1 cap', s_1.res_capacity)\n",
    "print('s_0 price', s_0.price)\n",
    "print('s_1 price', s_1.price)\n",
    "print('s_0 obj features', s_0.ObjectsFeatures)\n",
    "print('s_1 obj features', s_1.ObjectsFeatures)\n",
    "print('reward', reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation based on: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second big module: Model\n",
    "- the model contains the training/evaluation policy and it is responsable for choosing the actions\n",
    "- it contains the NN used for value function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs):    \n",
    "        \n",
    "        super(Net_Attention, self).__init__()\n",
    "        \n",
    "        self.emb = torch.nn.Linear(num_input_features,  h_model)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=h_model, nhead=num_head, \n",
    "                                                   dim_feedforward=dim_feedforward, dropout=p_dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "                \n",
    "        # FINAL LAYER\n",
    "        self.final_layer_0 = torch.nn.Linear(h_model,h_model)\n",
    "        self.final_layer_1 = torch.nn.Linear(h_model,1)\n",
    "\n",
    "    def forward(self, States, no_grad = False):\n",
    "        \n",
    "        ObjectsFeatures = [state.ObjectsFeatures for state in States]\n",
    "        ObjectsFeatures = torch.stack(ObjectsFeatures)\n",
    "        \n",
    "        # define relu operation\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        \n",
    "        if no_grad: #doesn't compute the gradients (hence no back propagation, but faster). \n",
    "            with torch.no_grad(): # to be used only in evaluations and target (not in training the prediction)        \n",
    "                E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "                # where \n",
    "                # S is the source sequence length,  \n",
    "                # N is the batch size,  \n",
    "                # E is the feature number        \n",
    "                # you want \n",
    "                # E.size() = S,N,E = num_obj x batch x h_model\n",
    "                c = self.transformer_encoder(E)\n",
    "                c = self.final_layer_0(c)\n",
    "                c = ReLU(c)\n",
    "                Qvals = self.final_layer_1(c)\n",
    "                Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "                Qvals = torch.transpose(Qvals,0,1)\n",
    "                \n",
    "                return Qvals\n",
    "            \n",
    "        E = self.emb(torch.transpose(ObjectsFeatures,0,1))\n",
    "        # where \n",
    "        # S is the source sequence length,  \n",
    "        # N is the batch size,  \n",
    "        # E is the feature number        \n",
    "        # you want \n",
    "        # E.size() = S,N,E = num_obj x batch x h_model\n",
    "        c = self.transformer_encoder(E)\n",
    "        c = self.final_layer_0(c)\n",
    "        c = ReLU(c)\n",
    "        Qvals = self.final_layer_1(c)\n",
    "        Qvals = torch.squeeze(Qvals, dim = 2)\n",
    "        Qvals = torch.transpose(Qvals,0,1)\n",
    "                \n",
    "        return Qvals\n",
    "\n",
    "class ModelClass():\n",
    "\n",
    "    def __init__(self, Net_prediction, Net_target, training_policy, evaluation_policy, epsilon_max, epsilon_min):\n",
    "        \n",
    "        self.Net_prediction = Net_prediction\n",
    "        self.Net_target     = Net_target\n",
    "        if training_policy!='eps-greedy' or evaluation_policy!='greedy':\n",
    "            print('at least one of this exploration/evaluation policy are still to implement')\n",
    "            error\n",
    "        else:\n",
    "            self.epsilon_min = epsilon_min\n",
    "            self.epsilon_max = epsilon_max\n",
    "        self.training_policy   = training_policy\n",
    "        self.evaluation_policy = evaluation_policy\n",
    "\n",
    "    def Greedy(self, Qvals, state):\n",
    "        \n",
    "        Qvals_selectable = [Qvals[i] for i in range(len(Qvals)) if i in state.IndexesActionsNotMasked]\n",
    "        Qvals_selectable = torch.stack(Qvals_selectable)\n",
    "        action = torch.argmax(Qvals_selectable)\n",
    "        action_index = state.IndexesActionsNotMasked[action]\n",
    "\n",
    "        return action_index\n",
    "    \n",
    "    def EpsGreedy(self, Qvals,state, iteration_counter, num_iterations):\n",
    "    \n",
    "        # select action\n",
    "        epsilon = (epsilon_max-epsilon_min)*(num_iterations-iteration_counter)/num_iterations + epsilon_min\n",
    "        if random.random() < epsilon:\n",
    "            action_index = random.choice(state.IndexesActionsNotMasked)\n",
    "        else:\n",
    "            action_index = self.Greedy(Qvals,state)\n",
    "\n",
    "        return action_index\n",
    "    \n",
    "    def training_step(self, state, iteration_counter, num_iterations):\n",
    "                \n",
    "        self.Net_prediction.train()\n",
    "        self.Net_target.train()\n",
    "        if self.training_policy == 'eps-greedy':\n",
    "            Qvals = self.Net_prediction([state])\n",
    "            Qvals = Qvals.squeeze()\n",
    "            return self.EpsGreedy(Qvals, state, iteration_counter, num_iterations)\n",
    "        error\n",
    "    \n",
    "    def evaluation_step(self, state):\n",
    "        \n",
    "        self.Net_prediction.eval()\n",
    "        if self.evaluation_policy == 'greedy':\n",
    "            Qvals = self.Net_prediction([state], no_grad = True)\n",
    "            Qvals = Qvals.squeeze()\n",
    "            return self.Greedy(Qvals, state)\n",
    "        error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that, given a state, it returns the Q values. We have to define the loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third module: BUFFER (aka replay memory)\n",
    "- the buffer contains the last #replay_lenght transictions.\n",
    "- each experience in the buffer is as a tuple <$s_{old}$, action, r, $s_{new}$>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemoryClass:\n",
    "    \n",
    "    replay_lenght = None\n",
    "    minibatch_size = None\n",
    "    Buffer = None\n",
    "    \n",
    "    def __init__(self,replay_lenght, minibatch_size):\n",
    "        \n",
    "        self.replay_lenght = replay_lenght\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.Buffer = []\n",
    "        \n",
    "    def Add(self, Transitions):\n",
    "                \n",
    "        while len(self.Buffer)+len(Transitions)>=self.replay_lenght:\n",
    "            self.Buffer.remove(random.choice(self.Buffer))\n",
    "        for t in Transitions:\n",
    "            self.Buffer.append(t)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def Minibatch(self, new_transactions = 0):\n",
    "        \n",
    "        if len(self.Buffer) <= self.minibatch_size:\n",
    "            Elements = self.Buffer\n",
    "        else:\n",
    "            new_transactions = min(new_transactions, self.minibatch_size)\n",
    "            last_transactions = self.Buffer[-new_transactions:]\n",
    "            Elements = random.sample(self.Buffer, self.minibatch_size - new_transactions)\n",
    "            if self.minibatch_size - new_transactions==0:\n",
    "                print('check that everything is fine and dandy')\n",
    "                print(len(last_transactions))\n",
    "                print(len(Elements))\n",
    "                stoptocheck\n",
    "            Elements = last_transactions + Elements\n",
    "            Minibatch = Elements\n",
    "        \n",
    "        # CHANGE THE WAY YOU SAVE THINFS IN THE BUFFER!!!!\n",
    "        # SAVE DIRECTLY AS THE LISTS BELOW\n",
    "        \n",
    "        States = [experience[0] for experience in Elements]\n",
    "        Actions = [experience[1] for experience in Elements]\n",
    "        Rewards = [experience[2] for experience in Elements]\n",
    "        New_States = [experience[3] for experience in Elements]\n",
    "        \n",
    "        Minibatch = [States, Actions, Rewards, New_States]\n",
    "        \n",
    "        return Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth module: LEARNER\n",
    "- receives minibatches\n",
    "- updates the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnerClass():\n",
    "\n",
    "    Model = None\n",
    "    optimizer = None\n",
    "    gamma = None\n",
    "    tau = None\n",
    "    \n",
    "    def __init__(self,Model, optimizer, tau, gamma = 1):\n",
    "        \n",
    "        self.Model = Model\n",
    "        self.optimizer = optimizer\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def Learning_step(self, Minibatch):\n",
    "    \n",
    "        # zero the gradient buffers\n",
    "        self.optimizer.zero_grad() \n",
    "\n",
    "        Losses = []\n",
    "        States, Actions, Rewards, New_States = Minibatch\n",
    "\n",
    "        New_States_Target = [New_States[i] for i in range(len(New_States)) if New_States[i].Final==0]\n",
    "        if len(New_States_Target)>0:\n",
    "            Max_New_States_Target, _ = torch.max(Model.Net_target(New_States_Target, no_grad = True), dim = 1)\n",
    "        else:\n",
    "            Max_New_States_Target = []\n",
    "        Max_New_States = []\n",
    "        j = 0\n",
    "        for i in range(len(New_States)):\n",
    "            if New_States[i].Final==0: # not final\n",
    "                Max_New_States.append(Max_New_States_Target[j])\n",
    "                j+=1\n",
    "            else:\n",
    "                zero_tensor_scalar = torch.tensor(0).float()\n",
    "                zero_tensor_scalar.requires_grad = False\n",
    "                Max_New_States.append(zero_tensor_scalar)\n",
    "        Max_New_States = torch.stack(Max_New_States)\n",
    "        Rewards = torch.tensor(Rewards)\n",
    "        New_States_Prediction = self.Model.Net_prediction(States)\n",
    "        Predictions_Actions = [New_States_Prediction[i][Actions[i]] for i in range(len(New_States_Prediction))]\n",
    "        Predictions_Actions = torch.stack(Predictions_Actions)\n",
    "\n",
    "        Losses = (Rewards + self.gamma*Max_New_States - Predictions_Actions)**2 \n",
    "\n",
    "        loss = Losses.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        clip_value = 1\n",
    "        for p in self.Model.Net_prediction.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n",
    "\n",
    "        # apply gradient\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft target update\n",
    "        for param_target, param_predict in zip(self.Model.Net_target.parameters(), self.Model.Net_prediction.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param_predict.data * self.tau)        \n",
    "        \n",
    "        return loss.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth module: RUNNER\n",
    "- takes the environment and the module\n",
    "- runs a transaction. Or should it run an episode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnerClass():\n",
    "\n",
    "    Environment = None\n",
    "    Model = None\n",
    "    \n",
    "    def __init__(self, Environment, Model):\n",
    "        \n",
    "        self.Model = Model\n",
    "        self.Environment = Environment\n",
    "        \n",
    "    def NewTrainingTask(self, iteration_counter,num_iterations):\n",
    "        \n",
    "        Transitions = []\n",
    "        state, task = self.Environment.InitialStateNewTask()\n",
    "        while state.Final == 0:\n",
    "            action_index = self.Model.training_step(state, iteration_counter,num_iterations)\n",
    "            new_state, reward = task.step(state, action_index)\n",
    "            Transitions.append([state, action_index, reward, new_state])\n",
    "            state = new_state\n",
    "         \n",
    "        return Transitions, task.target\n",
    "    \n",
    "    def NewEvaluationTask(self):\n",
    "        \n",
    "        state, task = self.Environment.InitialStateNewTask()\n",
    "        G = 0\n",
    "        while state.Final == 0:\n",
    "            action_index = self.Model.evaluation_step(state)\n",
    "            new_state, reward = task.step(state, action_index)\n",
    "            G+=reward\n",
    "            state = new_state\n",
    "         \n",
    "        return G, task.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function to evaluate the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluatePrint(R,T, PATH_Final, time_counter):\n",
    "    \n",
    "    R_norm = [R[i]/T[i] for i in range(len(R))]\n",
    "    T_norm = [1 for i in range(len(R))]\n",
    "        \n",
    "    if PATH_Final!=False:\n",
    "        plt.figure()\n",
    "        plt.plot(R)\n",
    "        plt.plot(R, label='Rewards')\n",
    "        plt.plot(T)\n",
    "        plt.plot(T, label='heuristic')\n",
    "        plt.ylabel(' Rewards ' )\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "\n",
    "        plt.savefig(PATH_Final+'Evaluation_Rewards_num_'+str(time_counter)+'.png', bbox_inches='tight')\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(R_norm)\n",
    "        plt.plot(R_norm, label='Rewards (normalized)')\n",
    "        plt.plot(T_norm)\n",
    "        plt.plot(T_norm, label='heuristic (normalized)')\n",
    "        plt.ylabel(' Rewards (normalized)' )\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)\n",
    "        \n",
    "        plt.savefig(PATH_Final+'Evaluation_Rewards_Normalized_num_'+str(time_counter)+'.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "    print('average norm reward (net)   : ', np.mean(R_norm))\n",
    "    if np.mean(R_norm)>np.mean(T_norm):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def Evaluate(num_iterations, Runner, times = 1, PATH_Final = False, examples = 0):\n",
    "    \n",
    "    better = 0\n",
    "    for time_counter in range(1,times+1):\n",
    "        print('evaluating model..('+str(time_counter)+' out of '+str(times)+')')\n",
    "        G_tot = []\n",
    "        heuristic_tot = []\n",
    "        for iteration_counter in range(num_iterations):\n",
    "            # perform new task\n",
    "            G, heuristic = Runner.NewEvaluationTask()\n",
    "            G_tot.append(G)\n",
    "            heuristic_tot.append(heuristic)\n",
    "        b = EvaluatePrint(G_tot,heuristic_tot, PATH_Final, time_counter)\n",
    "        better+=b\n",
    "    print('\\n\\n\\n')\n",
    "    print('the NN was better than the heuristic '+str(better)+' times out of '+str(times))                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our 5 modules (or load them from existing ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "random.seed(1234)\n",
    "# model dimensions\n",
    "num_iterations = 4*2500\n",
    "num_objs = 10\n",
    "num_input_features = 4 # prize, weight, res_capacity, Bool_alreday_chosen\n",
    "num_outputs = num_objs\n",
    "h_model = 16\n",
    "num_head = 4\n",
    "num_layers = 2\n",
    "dim_feedforward = 16\n",
    "p_dropout = 0.0\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "replay_lenght = 10e6\n",
    "minibatch_size = 32\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "tau = 0.005\n",
    "PATH = '/home/big_bamboo/RL for the KP/'\n",
    "PATH_Aux = '/home/big_bamboo/RL for the KP/Saved_objects_Aux/'\n",
    "PATH_Final = '/home/big_bamboo/RL for the KP/Final_Objects/'\n",
    "num_iterations_start = 0\n",
    "Q_print = []\n",
    "\n",
    "# is it a new test?\n",
    "LoadTest = False\n",
    "# objects in the middel of training (Aux) or completely computed (Final)\n",
    "Path_load = PATH_Aux\n",
    "if LoadTest:\n",
    "    #Net_predict.load_state_dict(torch.load(Path_load+'Net_Predict'))\n",
    "    #Net_target.load_state_dict(torch.load(Path_load+'Net_Target'))\n",
    "    with open(Path_load+'RLKP-num_iterations_start-Save.pkl', 'rb') as input:\n",
    "        num_iterations_start = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Learner-Save.pkl', 'rb') as input:\n",
    "        Learner = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Runner-Save.pkl', 'rb') as input:\n",
    "        Runner = pickle.load(input)\n",
    "    env = Runner.Environment\n",
    "    Model = Learner.Model\n",
    "    Runner.Model = Model\n",
    "    with open(Path_load+'RLKP-ReplayMemory-Save.pkl', 'rb') as input:\n",
    "        ReplayMemory = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Q_print-Save.pkl', 'rb') as input:\n",
    "        Q_print = pickle.load(input)\n",
    "else:\n",
    "    # environment\n",
    "    env = EnvironmentClass(num_objs)\n",
    "    # model\n",
    "    Net_predict = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "    Net_target = Net_Attention(num_input_features,h_model, num_head ,num_layers, dim_feedforward, p_dropout, num_outputs)\n",
    "    Model = ModelClass(Net_predict, Net_target, 'eps-greedy', 'greedy', epsilon_max, epsilon_min)\n",
    "    # Replay Memory\n",
    "    ReplayMemory = ReplayMemoryClass(replay_lenght, minibatch_size)\n",
    "    # Learner\n",
    "    optimizer = torch.optim.SGD(Net_predict.parameters(), lr=learning_rate, momentum = momentum)\n",
    "    Learner = LearnerClass(Model, optimizer, tau)\n",
    "    # Runner\n",
    "    Runner = RunnerClass(env, Model)\n",
    "\n",
    "def SaveData(iteration_counter,Learner,Runner,ReplayMemory,Q_print, PATH_save):\n",
    "    \n",
    "    #torch.save(Net_predict.state_dict(), PATH+'Net_Predict')\n",
    "    #torch.save(Net_target.state_dict(), PATH+'Net_Target')\n",
    "    with open(PATH_save+'RLKP-num_iterations_start-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(iteration_counter, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-Learner-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Learner, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-Runner-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Runner, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-ReplayMemory-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(ReplayMemory, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PATH_save+'RLKP-Q_print-Save.pkl', 'wb') as output:\n",
    "        pickle.dump(Q_print, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we also define a function to print the initial Q vales.\n",
    "(this function is not really smart, it recomputes the Q values. think of another way to do it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxInitialQvalues(Transitions, Net_predict, target_heuristic):\n",
    "    \n",
    "    first_state, action_index, reward, new_state = Transitions[0]\n",
    "    Qvals = Net_predict([first_state], no_grad = True)\n",
    "    Qvals = Qvals.squeeze().tolist()\n",
    "    \n",
    "    return [Qvals, target_heuristic]\n",
    "\n",
    "def PrintMaxQval(Q_prints_tuple, PATH_Final):\n",
    "    \n",
    "    Q_prints = [Q[0] for Q in Q_prints_tuple]\n",
    "    T = [Q[1] for Q in Q_prints_tuple]\n",
    "    \n",
    "    Q_max = [max(Q_vals) for Q_vals in Q_prints]\n",
    "    plt.figure()\n",
    "    for i in range(len(Q_prints[0])):\n",
    "        Q_Action_i = [Q_prints[j][i] for j in range(len(Q_prints))]\n",
    "        #plt.plot(Q_Action_i)\n",
    "        plt.plot(Q_Action_i, label='Q_Action_'+str(i))\n",
    "    plt.plot(Q_max, label='Q_max')\n",
    "    plt.legend()\n",
    "    plt.savefig(PATH_Final+'Q_Vals_Training.png', bbox_inches='tight')\n",
    "    \n",
    "    # same but normalized\n",
    "    Q_max = [Q_max[i]/T[i] for i in range(len(Q_max))]\n",
    "    plt.figure()\n",
    "    for i in range(len(Q_prints[0])):\n",
    "        Q_Action_i = [Q_prints[j][i]/T[j] for j in range(len(Q_prints))]\n",
    "        #plt.plot(Q_Action_i)\n",
    "        plt.plot(Q_Action_i, label='Q_Action_Norm_'+str(i))\n",
    "    plt.plot(Q_max, label='Q_Max_Norm')\n",
    "    Ones = [1 for j in range(len(Q_prints))]\n",
    "    plt.plot(Ones)\n",
    "    plt.legend()\n",
    "    plt.savefig(PATH_Final+'Q_Vals_Training_Norm.png', bbox_inches='tight')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task number:  0\n",
      "task number:  100\n",
      "task number:  200\n",
      "task number:  300\n",
      "task number:  400\n",
      "task number:  500\n",
      "task number:  600\n",
      "task number:  700\n",
      "task number:  800\n",
      "task number:  900\n",
      "task number:  1000\n",
      "task number:  1100\n",
      "task number:  1200\n",
      "task number:  1300\n",
      "task number:  1400\n",
      "task number:  1500\n",
      "task number:  1600\n",
      "task number:  1700\n",
      "task number:  1800\n",
      "task number:  1900\n",
      "task number:  2000\n",
      "task number:  2100\n",
      "task number:  2200\n",
      "task number:  2300\n",
      "task number:  2400\n",
      "task number:  2500\n",
      "task number:  2600\n",
      "task number:  2700\n",
      "task number:  2800\n",
      "task number:  2900\n",
      "task number:  3000\n",
      "task number:  3100\n",
      "task number:  3200\n",
      "task number:  3300\n",
      "task number:  3400\n",
      "task number:  3500\n",
      "task number:  3600\n",
      "task number:  3700\n",
      "task number:  3800\n",
      "task number:  3900\n"
     ]
    }
   ],
   "source": [
    "for iteration_counter in range(num_iterations_start, num_iterations):\n",
    "    if iteration_counter%100==0:\n",
    "        SaveData(iteration_counter,Learner,Runner,ReplayMemory,Q_print, PATH_Aux)\n",
    "        print('task number: ',(iteration_counter))\n",
    "    # perform new task\n",
    "    Transitions, target_heuristic = Runner.NewTrainingTask(iteration_counter, num_iterations)\n",
    "    Q_print.append(MaxInitialQvalues(Transitions, Model.Net_prediction, target_heuristic))\n",
    "    # save transition\n",
    "    ReplayMemory.Add(Transitions)\n",
    "    # learn        \n",
    "    if iteration_counter>= ReplayMemory.minibatch_size:\n",
    "        Learner.Learning_step(ReplayMemory.Minibatch())\n",
    "\n",
    "SaveData(num_iterations,Learner,Runner,ReplayMemory,Q_print, PATH_Final)\n",
    "PrintMaxQval(Q_print, PATH_Final)\n",
    "print('evaluation: ')\n",
    "Evaluate(100, Runner, PATH_Final = PATH_Final, times = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it a new test?\n",
    "LoadTest = True\n",
    "# objects in the middel of training (Aux) or completely computed (Final)\n",
    "Path_load = PATH_Aux\n",
    "Path_load = PATH_Final\n",
    "if LoadTest:\n",
    "    #Net_predict.load_state_dict(torch.load(Path_load+'Net_Predict'))\n",
    "    #Net_target.load_state_dict(torch.load(Path_load+'Net_Target'))\n",
    "    with open(Path_load+'RLKP-num_iterations_start-Save.pkl', 'rb') as input:\n",
    "        num_iterations_start = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Learner-Save.pkl', 'rb') as input:\n",
    "        Learner = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Runner-Save.pkl', 'rb') as input:\n",
    "        Runner = pickle.load(input)\n",
    "    env = Runner.Environment\n",
    "    Model = Learner.Model\n",
    "    Runner.Model = Model\n",
    "    with open(Path_load+'RLKP-ReplayMemory-Save.pkl', 'rb') as input:\n",
    "        ReplayMemory = pickle.load(input)\n",
    "    with open(Path_load+'RLKP-Q_print-Save.pkl', 'rb') as input:\n",
    "        Q_print = pickle.load(input)\n",
    "Evaluate(100, Runner, times = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- check if it works\n",
    "- prioritized replay is actually working.\n",
    "\n",
    "ALSO:\n",
    "- lr scheduler\n",
    "- should episodes been created in batches? If so, how to 'schedlue' them in the replay buffer? (The idea is to pick the last one in the replay, but how to do this if you generate X episodes at a time?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to train\n",
    "\n",
    "# optimizer scheduler\n",
    "#lmbda = lambda epoch: 1\n",
    "#scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "#if (e_counter)%int(num_epoch/5)==0 and e_counter>minibatch_size:\n",
    "#    scheduler.step()\n",
    "#    for param_group in optimizer.param_groups:\n",
    "#        print('learning rate ', param_group['lr'])\n",
    "#        if param_group['lr']<1e-10:\n",
    "#            print('the lr dropped too low, something is wrong')\n",
    "#            error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
