{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, I try to use RL to solve the knapsack problem.\n",
    "This should not be an exciting project but just a warm up (for me at least) to understand hands-on RL, RNN, embeddings and such.\n",
    "\n",
    "The project starts with:\n",
    "- a nn to move from 2 features per object to 8\n",
    "- a LTSM with an internal state composed by 5 features\n",
    "- its output is fed to a fully connected layer that from 5 feature expands to the number of objects\n",
    "- a softmax decides which element to choose\n",
    "- If you chose objects whose weight is more than 1, the reward is -100\n",
    "- attention mechanism with glimpse\n",
    "\n",
    "Still to implement:\n",
    "- mask to not choose 'wrong' objects\n",
    "\n",
    "As always, we start by importing useful libraries and defiening some boundary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "import random\n",
    "import itertools    \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(1234)\n",
    "prize_min = 0\n",
    "prize_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ObjectCass:\n",
    "    \n",
    "    Prize = None\n",
    "    Weight = None\n",
    "    \n",
    "    def __init__(self, reward, weight):\n",
    "        \n",
    "        self.Prize = reward\n",
    "        self.Weight = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the function to generate new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GenerateANewInstance(num_objs):\n",
    "    \n",
    "    Objects = []\n",
    "    for i in range(num_objs):\n",
    "        prize = round(random.uniform(prize_min, prize_max), 0)\n",
    "        weight = round(random.uniform(0, 1), 2)\n",
    "        obj = ObjectCass(prize,weight)\n",
    "        Objects.append(obj)\n",
    "    Objects.append(ObjectCass(0,0)) # fake object representing that you don't pick up anything\n",
    "                                   # i.e. you stop picking up stuff\n",
    "    \n",
    "    return Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a very general agent --  NOT USED!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AgentClass:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        \n",
    "        current_obs = env.get_observation() #this could be rmeoved I think\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        return action\n",
    "Agent = AgentClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I try to create a shallow NN. from the features of the input (which are 2), I want to output 10 features (whith one hidden layer of size 10 as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class EmbeddingPlusLSTM(torch.nn.Module):\n",
    "#    \n",
    "#    def __init__(self, num_input, num_embeddings, num_h, num_output, batch_size):    \n",
    "#        \n",
    "#        super(EmbeddingPlusLSTM, self).__init__() # this is not really clear to me\n",
    "#        \n",
    "#        self.fc1 = torch.nn.Linear(num_input, num_embeddings)\n",
    "#        self.fc2 = torch.nn.Linear(num_embeddings, num_embeddings)\n",
    "#        self.lstm= torch.nn.LSTM(num_embeddings,  num_h)\n",
    "#        self.fc3 = torch.nn.Linear(num_h, num_output)\n",
    "#        \n",
    "#        # here I set the first internal state of the RNN.\n",
    "#        # I set it here and not in forwards otherwise it start with different h_0\n",
    "#        # For now it's zero-initialied, but maybe there is something smarter \n",
    "#        # that can be done.\n",
    "#        \n",
    "#        self.batch_size = batch_size \n",
    "#        n_layers = 1                \n",
    "#        self.hidden_state = torch.zeros(n_layers, self.batch_size, self.fc3.in_features)\n",
    "#        self.cell_state = torch.zeros(n_layers, self.batch_size, self.fc3.in_features)\n",
    "#            \n",
    "#    def forward(self, x):\n",
    "#        \n",
    "#        self.hidden = (self.hidden_state, self.cell_state)\n",
    "#        x = self.fc1(x)\n",
    "#        x = self.fc2(x)\n",
    "#        \n",
    "#        x = x[None, :, :] # I don't know why you have to add a dimension here\n",
    "#        \n",
    "#        self.hidden, cell_state = self.lstm(x, self.hidden)\n",
    "#        x = self.fc3(self.hidden)  \n",
    "#        x = torch.nn.functional.softmax(x, dim = 1)\n",
    "#        \n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tensors = torch.tensor([[1., -1.],[2., 15.],[1., -1.],[20., 12.],[1., -1.],[20., 12.],[1., -1.],[20., 12.]])\n",
    "#Net = EmbeddingPlusLSTM(2, 8, 5, 10, len(tensors)) \n",
    "## 2 is the number of features of an Object\n",
    "## 8 is the number of features for the embedding\n",
    "## 5 number of features for the hidden size of the rnn\n",
    "##10 is the number of output features\n",
    "## the last one is the batch size\n",
    "#\n",
    "#print('number of parameters: ', sum(p.numel() for p in Net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we define the parameters of the actor network and we create it. We also take a look at the number of trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features, dim_kernel, num_embeddings, num_h, dim_W_a, dim_W_c, batch_size):    \n",
    "        \n",
    "        super(RNN_Attention, self).__init__() # this is not really clear to me\n",
    "        \n",
    "        self.cnn_embedding = torch.nn.Conv1d(num_input_features, num_embeddings, dim_kernel) # in channel, out channel, kernel size\n",
    "        print('still to decide the kernel size')\n",
    "        print('and how do you set the number of filters to D?')\n",
    "        print('so far I set the number of groups to num_input_features')\n",
    "        print('doing so, each input featur has its own set of filters (see documentation)')\n",
    "        self.lstm= torch.nn.LSTM(num_embeddings,  num_h)        \n",
    "        self.last_item_embedding = torch.nn.Linear(num_input_features,  num_embeddings)        \n",
    "        print('also, so far you feed to the LSTM both the static and dinamic info.')\n",
    "        print('and to match the dimension, you pass this info into a fully connected layer')\n",
    "        print('you will have to change it, so to feed only the static part')\n",
    "        self.W_a = torch.tensor(torch.rand(dim_W_a,num_embeddings + num_h), requires_grad=True)\n",
    "        self.v_a = torch.tensor(torch.rand(dim_W_a), requires_grad=True)\n",
    "        self.v_a = self.v_a[None, :] # I don't know why you have to add a dimension here\n",
    "        self.W_c = torch.tensor(torch.rand(dim_W_c, 2*num_embeddings), requires_grad=True)\n",
    "        self.v_c = torch.tensor(torch.rand(dim_W_c), requires_grad=True)\n",
    "        self.v_c = self.v_c[None, :] # I don't know why you have to add a dimension here\n",
    "        # last item defines the last item you selected. \n",
    "        # it is initialized as an empty item.\n",
    "        \n",
    "        # here I set the first internal state of the RNN.\n",
    "        # I set it here and not in forwards otherwise it start with different h_0\n",
    "        # For now it's zero-initialied, but maybe there is something smarter \n",
    "        # that can be done.\n",
    "        \n",
    "        self.batch_size = batch_size \n",
    "        n_layers = 1                \n",
    "        self.hidden_state = torch.zeros(n_layers, self.batch_size, num_h)\n",
    "        self.cell_state = torch.zeros(n_layers, self.batch_size, num_h)\n",
    "            \n",
    "    def forward(self, x, last_item):\n",
    "\n",
    "        # also this is to change (why it does not work if I assign the fucntion directly?)\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        # compuitng h_t\n",
    "        last_item = self.last_item_embedding(torch.tensor([last_item.Prize, last_item.Weight]).float())\n",
    "        last_item = ReLU(last_item)\n",
    "        last_item = last_item[None, None, :] # I don't know why you have to add a dimension here\n",
    "        h_t, cell_t = self.lstm( last_item)  # if you do not provide the initial cell and state, they are by default zero.\n",
    "        h_t = torch.squeeze(h_t, dim = 0)    # removing the dimension seq_len since I don't know what it does\n",
    "        h_t = h_t.t()             # transposing so i have features x samples\n",
    "        # computing x_bar\n",
    "        x = x[:, :, None] # I don't know why you have to add a dimension here\n",
    "        x_bar = self.cnn_embedding(x)\n",
    "        # concatenation of two tensors\n",
    "        Concatenations = []\n",
    "        for x_bar_i in x_bar:\n",
    "            Concatenations.append(torch.cat((x_bar_i, h_t), 0))\n",
    "        concatenation_1 = torch.stack(Concatenations, dim = 0)\n",
    "        concatenation_1 = torch.squeeze(concatenation_1, dim = 2)\n",
    "        concatenation_1 = concatenation_1.t()\n",
    "        # computing u_t\n",
    "        u_t = torch.mm(self.v_a,torch.tanh(torch.mm(self.W_a, concatenation_1))).t()  \n",
    "        # computing a_t\n",
    "        a_t = torch.nn.functional.softmax(u_t, dim = 0)        \n",
    "        # computing the context vector c_t\n",
    "        C_t_array = []\n",
    "        index = 0\n",
    "        for row in x_bar: # apaprently with split you use less grad operations\n",
    "            C_t_array.append(torch.mul(row, a_t[index]))\n",
    "            index+=1        \n",
    "        c_t = torch.stack(C_t_array,dim=0)\n",
    "        c_t = torch.sum(c_t, dim=0)        \n",
    "        # concatenation of two tensors\n",
    "        Concatenations_2 = []\n",
    "        for x_bar_i in x_bar:\n",
    "            Concatenations_2.append(torch.cat((x_bar_i, c_t), 0))\n",
    "        concatenation_2 = torch.stack(Concatenations_2, dim = 0)\n",
    "        concatenation_2 = torch.squeeze(concatenation_2, dim = 2).t()        \n",
    "        # computing u_t_tilde\n",
    "        u_t_tilde = torch.mm(self.v_c,torch.tanh(torch.mm(self.W_c, concatenation_2)))\n",
    "        # computing actual output\n",
    "        x = torch.nn.functional.softmax(u_t_tilde, dim = 1)\n",
    "        x = torch.squeeze(x, dim = 0)\n",
    "\n",
    "        return x, x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still to decide the kernel size\n",
      "and how do you set the number of filters to D?\n",
      "so far I set the number of groups to num_input_features\n",
      "doing so, each input featur has its own set of filters (see documentation)\n",
      "also, so far you feed to the LSTM both the static and dinamic info.\n",
      "and to match the dimension, you pass this info into a fully connected layer\n",
      "you will have to change it, so to feed only the static part\n",
      "('number of parameters: ', 348)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BigBamboo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/BigBamboo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/BigBamboo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/BigBamboo/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4990, 0.0010, 0.4990, 0.0010], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#create an instance\n",
    "num_objects = 10\n",
    "Objects = GenerateANewInstance(num_objects)\n",
    "for obj in Objects:\n",
    "    if obj.Prize == 0 and obj.Weight == 0:\n",
    "        last_item = obj\n",
    "\n",
    "\n",
    "num_input_features = 2 # number of features of an Object\n",
    "dim_kernel = 1         # kernel dimension (since there are only two features, kerneal dimension = 1)\n",
    "num_embeddings = 8     # number of features for the embedding\n",
    "num_h = 5              # number of features for the hidden size of the rnn\n",
    "dim_W_a = 4            # free dimension of matrix W_a\n",
    "dim_W_c = 6            # free dimension of matrix W_c\n",
    "tensors = torch.tensor([[1., -1.],[2., 15.],[1., -1.],[20., 12.]])\n",
    "#tensors = torch.tensor([[1., -1.]])\n",
    "Net = RNN_Attention(num_input_features, dim_kernel, num_embeddings, num_h, dim_W_a, dim_W_c, len(tensors))\n",
    "\n",
    "print('number of parameters: ', sum(p.numel() for p in Net.parameters() if p.requires_grad))\n",
    "output, x_bar = Net(tensors, last_item)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before implementing the actual RL framework, we need to define the critic network. The critic network takes as input the probabilities of the actor network (after its masking) and the embeddings input (x_bar). These two are multiplied to obtain a weighted sum of the embedded inputs then they are fed into a network with 2 layers. The first layer is dense with ReLU activation and the second one is a linear layer with single output (which is the expected reward). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input_features, num_inbetween_features):    \n",
    "        \n",
    "        super(Critic, self).__init__() # this is not really clear to me\n",
    "        self.FirstLayer = torch.nn.Linear(num_input_features, num_inbetween_features)\n",
    "        self.SecondLayer = torch.nn.Linear(num_inbetween_features,  1)        \n",
    "            \n",
    "    def forward(self, x_bar, probabilities):\n",
    "\n",
    "        # also this is to change (why it does not work if I assign the fucntion directly?)\n",
    "        ReLU = torch.nn.ReLU()\n",
    "        i = 0\n",
    "        for x_bar_i in x_bar:\n",
    "            if i == 0:\n",
    "                weighted_sum = x_bar_i*probabilities[0]\n",
    "            else:\n",
    "                weighted_sum+= x_bar_i*probabilities[0]\n",
    "            i+=1\n",
    "        weighted_sum = weighted_sum.squeeze()\n",
    "        output = self.FirstLayer(weighted_sum)\n",
    "        output = ReLU(output)\n",
    "        output = self.SecondLayer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of critic parameters: ', 161)\n"
     ]
    }
   ],
   "source": [
    "num_inbetween_features = 16\n",
    "Net_critic = Critic(num_embeddings, num_inbetween_features)\n",
    "print('number of critic parameters: ', sum(p.numel() for p in Net_critic.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGE LAST_ITEM SUCH THAT IT ALSO EMBEDDS THE REMAINING AVAILABLE CAPACITY\n",
      "change the mask such that it does not even feed old element to the Net\n",
      "check again the probability thing.\n",
      "I don't think it's the probability of the last action but the probability of the whole sequence\n",
      "('iteration number ', 0)\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ca3cfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('CHANGE LAST_ITEM SUCH THAT IT ALSO EMBEDDS THE REMAINING AVAILABLE CAPACITY')\n",
    "print('change the mask such that it does not even feed old element to the Net')\n",
    "print('check again the probability thing.')\n",
    "print('I don\\'t think it\\'s the probability of the last action but the probability of the whole sequence')\n",
    "\n",
    "# define for how many different instances it should run, their size and the number of epochs\n",
    "num_instances = 1\n",
    "num_objs = 10\n",
    "num_epoch = 100\n",
    "\n",
    "# define the loss criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "# create your optimizers\n",
    "optimizer_actor = torch.optim.SGD(Net.parameters(), lr=0.001)\n",
    "optimizer_critic = torch.optim.SGD(Net_critic.parameters(), lr=0.001)\n",
    "Rewards = []\n",
    "Loss_actor = []\n",
    "Loss_critic = []\n",
    "\n",
    "# clipping the gradients\n",
    "clip_value = 2\n",
    "torch.nn.utils.clip_grad_value_(Net.parameters(), clip_value)\n",
    "torch.nn.utils.clip_grad_value_(Net_critic.parameters(), clip_value)\n",
    "\n",
    "\n",
    "for e_counter in range(num_epoch):\n",
    "    for i_counter in range(num_instances):\n",
    "        Objects = GenerateANewInstance(num_objs)\n",
    "        ObjectsFeatures = torch.tensor([[obj.Prize, obj.Weight] for obj in Objects])\n",
    "        last_item = Objects[-1]\n",
    "        Chosen = []\n",
    "        total_reward = 0.0\n",
    "        total_weight = 0\n",
    "        total_steps = 0\n",
    "        while total_steps < num_objs + 1:\n",
    "            total_steps += 1\n",
    "            output, x_bar = Net(ObjectsFeatures,last_item)\n",
    "            if False:\n",
    "                # mask to not re-choose old element\n",
    "                for el in Chosen:\n",
    "                    output[Objects.index(el)]=0\n",
    "                # mask to not choose element with too much weight\n",
    "                for el in Objects:\n",
    "                    if el not in Chosen:\n",
    "                        if total_weight + el.Weight > 1:\n",
    "                            output[Objects.index(el)]=0\n",
    "            # choose action\n",
    "            action_index = torch.argmax(output)\n",
    "            # perform action\n",
    "            total_weight+=Objects[action_index].Weight\n",
    "            total_reward+=Objects[action_index].Prize\n",
    "            Chosen.append(Objects[action_index])\n",
    "            last_item = Objects[action_index]\n",
    "            if total_weight > 1:\n",
    "                total_reward = -10\n",
    "                break\n",
    "            if last_item.Prize == 0 and last_item.Weight == 0:\n",
    "                total_reward = 100\n",
    "                break\n",
    "        Rewards.append(total_reward)\n",
    "    \n",
    "    optimizer_actor.zero_grad()   # zero the gradient buffers\n",
    "    optimizer_critic.zero_grad()  # zero the gradient buffers\n",
    "    # observe reward critic network \n",
    "    target = Net_critic(x_bar, output)\n",
    "    # loss critic\n",
    "    loss_critic = criterion(-torch.tensor(total_reward), target)\n",
    "    loss_critic = loss_critic/num_instances\n",
    "    Loss_critic.append(loss_critic.tolist())\n",
    "    loss_critic.backward(retain_graph=True)\n",
    "    # loss actor\n",
    "    loss_actor = (torch.tensor(total_reward) - target)*torch.log(output)\n",
    "    loss_actor = loss_actor.mean()\n",
    "    Loss_actor.append(loss_actor.tolist())\n",
    "    # update actor\n",
    "    loss_actor.backward()\n",
    "    optimizer_critic.step()    # Does the update\n",
    "    optimizer_actor.step()    # Does the update\n",
    "    \n",
    "    if e_counter % 100 == 0:\n",
    "        print('iteration number ', e_counter)\n",
    "\n",
    "R = []\n",
    "A = []\n",
    "C = []\n",
    "for i in range(len(Rewards)):\n",
    "    if Rewards.index(Rewards[i])%10 == 0:\n",
    "        R.append(Rewards[i])\n",
    "        A.append(Loss_actor[i])\n",
    "        C.append(Loss_critic[i])\n",
    "#plt.figure()\n",
    "#plt.plot(R)\n",
    "#plt.figure()\n",
    "#plt.plot(A)\n",
    "#plt.figure()\n",
    "plt.plot(C)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36workshop",
   "language": "python",
   "name": "p36workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
